{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from vocab import Vocab\n",
    "from constants import *\n",
    "NDC_ATC_MAPPING = \"ndc_map.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline\">Statistics</span>\n",
    "This is the file where statistics are calculated\n",
    "\n",
    "#### <u>Stat-1: Method to calculate % of visits that have higher than certain threshold % of \"[unk]\" tokens </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_visits_with_high_unk(vocab: Vocab, threshold: float = 0.3, for_single_vsit: bool = False):\n",
    "    visit_record_key = [\"SUBJECT_ID\", \"HADM_ID\", \"ICD9_CODE\", \"ATC4\"]\n",
    "    visit = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_PKL))[visit_record_key[1:]]\n",
    "    if for_single_vsit is True:\n",
    "        visit = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, SINGLE_VISIT_PKL))[visit_record_key[1:]]\n",
    "    total = visit.shape[0]\n",
    "    def process(row):\n",
    "        icd: list[str] = row[\"ICD9_CODE\"]\n",
    "        unk = 0\n",
    "        for code in icd:\n",
    "            if vocab.word2idx.get(code) == None:\n",
    "                unk += 1\n",
    "        if unk/len(icd) > threshold:\n",
    "            print(row[\"HADM_ID\"])\n",
    "            return 1\n",
    "    temp = visit.apply(process, axis=1)\n",
    "    count: int = temp.loc[temp == 1].shape[0]\n",
    "    return count/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Stat-2: Method to calculate stats of Table 2 from Original paper </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[table_2_stats] # of patients (Single-Visit) 29189\n",
      "[table_2_stats] avg. # of visits (Single-Visit) 1.0\n",
      "[table_2_stats] avg. # of dx per visit per patient (Single-Visit) 11.444276953646922\n",
      "[table_2_stats] avg. # of rx per visit per patient (Single-Visit) 21.20703004556511\n",
      "[table_2_stats] unique # of dx (Single-Visit) 6191\n",
      "[table_2_stats] unique # of drx (Single-Visit) 398\n",
      "\n",
      "\n",
      "[table_2_stats] # of patients (Multi-Visit) 5917\n",
      "[table_2_stats] avg. # of visits (Multi-Visit) 2.677539293560926\n",
      "[table_2_stats] avg. # of dx per visit per patient (Multi-Visit)(This includes the first visit dx as well) 14.217572429464116\n",
      "[table_2_stats] avg. # of rx per visit per patient (Multi-Visit)(This includes the first visit rx as well) 22.555071640472132\n",
      "[table_2_stats] unique # of dx (Multi-Visit) 4551\n",
      "[table_2_stats] unique # of rx (Multi-Visit) 385\n"
     ]
    }
   ],
   "source": [
    "def table_2_stats():\n",
    "    visit_record_key = [\"SUBJECT_ID\", \"HADM_ID\", \"ICD9_CODE\", \"ATC4\"]\n",
    "    single_visit = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, SINGLE_VISIT_PKL))\n",
    "    multi_visit_temporal = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_TEMPORAL_PKL))\n",
    "    multi_visit = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_PKL))\n",
    "\n",
    "    # Single visit stats\n",
    "    single_visit_sbj_count = single_visit[visit_record_key[0]].nunique()\n",
    "    print(\"[table_2_stats] # of patients (Single-Visit)\", single_visit_sbj_count)\n",
    "    total_visits = single_visit[visit_record_key[:2]].groupby(\"SUBJECT_ID\").count().sum().values[0]\n",
    "    print(\"[table_2_stats] avg. # of visits (Single-Visit)\", total_visits/single_visit_sbj_count)\n",
    "    total_dx = single_visit[visit_record_key[2]].apply(len).sum()\n",
    "    print(\"[table_2_stats] avg. # of dx per visit per patient (Single-Visit)\", total_dx/total_visits)\n",
    "    total_rx = single_visit[visit_record_key[3]].apply(len).sum()\n",
    "    print(\"[table_2_stats] avg. # of rx per visit per patient (Single-Visit)\", total_rx/total_visits)\n",
    "    unique_dx = len(set(code for ls in single_visit[visit_record_key[2]] for code in ls))\n",
    "    print(\"[table_2_stats] unique # of dx (Single-Visit)\", unique_dx) # 7k since we do not filter patient data\n",
    "    # we replace in training/eval less frequent DX with \"[UNK]\" && similar for RX\n",
    "    unique_rx = len(set(code for ls in single_visit[visit_record_key[3]] for code in ls))\n",
    "    print(\"[table_2_stats] unique # of drx (Single-Visit)\", unique_rx)\n",
    "\n",
    "    # Multiple visit stats\n",
    "    multi_visit_sbj_count = multi_visit_temporal[visit_record_key[0]].nunique()\n",
    "    print(\"\\n\\n[table_2_stats] # of patients (Multi-Visit)\", multi_visit_sbj_count)\n",
    "    total_multi_visits = (multi_visit_temporal.groupby(by=[\"SUBJECT_ID\"])[\"T_1\"].max() + 1).sum()\n",
    "    print(\"[table_2_stats] avg. # of visits (Multi-Visit)\", total_multi_visits/multi_visit_sbj_count)\n",
    "    def process_icd(row: pd.Series):\n",
    "        row[\"ICD9_LEN\"] = sum([len(ls) for ls in row[\"ICD9_CODE\"]])\n",
    "        return row\n",
    "    # this can be improved but not P1    \n",
    "    total_multi_dx = multi_visit_temporal.apply(process_icd, axis=1).groupby(\"SUBJECT_ID\")[\"ICD9_LEN\"].max().sum() \n",
    "    print(\"[table_2_stats] avg. # of dx per visit per patient (Multi-Visit)(This includes the first visit dx as well)\"\\\n",
    "        , total_multi_dx/total_multi_visits) # though it is fall as seen in paper but it is because we are including both \n",
    "    def process_atc(row: pd.Series):\n",
    "        row[\"ATC4_LEN\"] = sum([len(ls) for ls in row[\"ATC4\"]])\n",
    "        return row\n",
    "    # this can be improved but not P1    \n",
    "    total_multi_rx = multi_visit_temporal.apply(process_atc, axis=1).groupby(\"SUBJECT_ID\")[\"ATC4_LEN\"].max().sum() \n",
    "    print(\"[table_2_stats] avg. # of rx per visit per patient (Multi-Visit)(This includes the first visit rx as well)\"\\\n",
    "        , total_multi_rx/total_multi_visits) # though it is fall as seen in paper but it is because we are including both \n",
    "    unique_dx_multi = set()\n",
    "    def store_unique_icd(row: pd.Series):\n",
    "        unique_dx_multi.update(code for ls in row[\"ICD9_CODE\"] for code in ls)\n",
    "        return row\n",
    "    multi_visit_temporal.apply(store_unique_icd, axis=1) \n",
    "    print(\"[table_2_stats] unique # of dx (Multi-Visit)\", len(unique_dx_multi))\n",
    "    unique_rx_multi = set()\n",
    "    def store_unique_atc(row: pd.Series):\n",
    "        unique_rx_multi.update(code for ls in row[\"ATC4\"] for code in ls)\n",
    "        return row\n",
    "    multi_visit_temporal.apply(store_unique_atc, axis=1) \n",
    "    print(\"[table_2_stats] unique # of rx (Multi-Visit)\", len(unique_rx_multi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Stat-3: Method to calculate stats of Table 2 from Original paper using csv from MIMIC-III</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mapping_file():\n",
    "    ndc_mapping = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, NDC_ATC_R_SCRIPT_MAPPING)).dropna()\\\n",
    "        .drop_duplicates()\n",
    "    def process_row(row: pd.Series):\n",
    "        row[\"NDC\"]= float(str(row[\"NDC\"]).replace(\"-\", \"\"))\n",
    "        return row\n",
    "    ndc_mapping = ndc_mapping.apply(process_row, axis=1).dropna().drop_duplicates()\n",
    "    ndc_mapping.to_csv(os.path.join(GLOBAL_DATA_PATH, NDC_ATC_MAPPING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[table_2_stats] # of patients (Single-Visit) 32212\n",
      "[table_2_stats] avg. # of visits (Single-Visit) 1.0\n",
      "[table_2_stats] avg. # of dx per visit per patient (Single-Visit) 10.985409164286601\n",
      "[table_2_stats] avg. # of rx per visit per patient (Single-Visit) 18.816621134980753\n",
      "[table_2_stats] unique # of dx (Single-Visit) 6278\n",
      "[table_2_stats] unique # of drx (Single-Visit) 294\n",
      "\n",
      "\n",
      "[table_2_stats] # of patients (Multi-Visit) 7151\n",
      "[table_2_stats] avg. # of visits (Multi-Visit) 2.6785065025870507\n",
      "[table_2_stats] avg. # of dx per visit per patient (Multi-Visit)(This includes the first visit dx as well) 13.318314712331627\n",
      "[table_2_stats] avg. # of rx per visit per patient (Multi-Visit)(This includes the first visit rx as well) 19.283543907277853\n",
      "[table_2_stats] unique # of dx (Multi-Visit) 4852\n",
      "[table_2_stats] unique # of rx (Multi-Visit) 292\n"
     ]
    }
   ],
   "source": [
    "def table_2_stats_using_csv():\n",
    "    visit_record_key = [\"SUBJECT_ID\", \"HADM_ID\", \"ICD9_CODE\", \"ATC4\"]\n",
    "    admissions = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, ADMISSIONS))[visit_record_key[:2] + [\"ADMITTIME\"]]\\\n",
    "        .dropna().drop_duplicates()\n",
    "    diagnosis_icd = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, DIAGNOSIS_ICD))[visit_record_key[:3]].dropna()\\\n",
    "        .drop_duplicates()\n",
    "    prescriptions = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, PRESCRIPTIONS))[[\"SUBJECT_ID\", \"HADM_ID\", \"NDC\"]]\n",
    "    ndc_mapping = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, NDC_ATC_MAPPING)).dropna()\\\n",
    "        .drop_duplicates()\n",
    "        \n",
    "    # Single visit stats\n",
    "    # Intersecting with prescriptions table since some patients have admission but no medications\n",
    "    # we are not concerned about them for this experiment\n",
    "    intersection = admissions.loc[admissions[\"SUBJECT_ID\"].isin(prescriptions[\"SUBJECT_ID\"])]\n",
    "    work_table_1 = intersection.groupby([visit_record_key[0]])[visit_record_key[1]].count() #SUBJECT_ID, HADM_ID\n",
    "    single_visit_sbj = work_table_1.loc[work_table_1 == 1].reset_index()[visit_record_key[0]] #SUBJECT_ID\n",
    "    print(\"[table_2_stats] # of patients (Single-Visit)\", single_visit_sbj.shape[0])\n",
    "    total_visits = work_table_1[single_visit_sbj].sum()\n",
    "    print(\"[table_2_stats] avg. # of visits (Single-Visit)\", total_visits/single_visit_sbj.shape[0])\n",
    "    work_table_2 = pd.merge(single_visit_sbj, diagnosis_icd, on=[\"SUBJECT_ID\"]).dropna().drop_duplicates() #SUBJECT_ID, HADM_ID, ICD9_CODE\n",
    "    total_dx = work_table_2[\"ICD9_CODE\"].shape[0]\n",
    "    print(\"[table_2_stats] avg. # of dx per visit per patient (Single-Visit)\", total_dx/total_visits)\n",
    "    work_table_3 = pd.merge(prescriptions, ndc_mapping, on=[\"NDC\"])[visit_record_key[:2] + [\"ATC4\"]] # SUBJECT_ID, HADM_ID\n",
    "    work_table_3 = work_table_3.dropna().drop_duplicates() # since repeated ATC4 codes for same SUBJECT_ID and HADM_ID\n",
    "    # ATC4\n",
    "    work_table_4 = pd.merge(work_table_3, single_visit_sbj, on=[\"SUBJECT_ID\"])\n",
    "    total_rx = work_table_4.shape[0]\n",
    "    print(\"[table_2_stats] avg. # of rx per visit per patient (Single-Visit)\", total_rx/total_visits)\n",
    "    unique_dx = work_table_2[\"ICD9_CODE\"].dropna().nunique()\n",
    "    print(\"[table_2_stats] unique # of dx (Single-Visit)\", unique_dx) # 7k since we do not filter patient data\n",
    "    unique_rx = work_table_4[\"ATC4\"].dropna().nunique()\n",
    "    print(\"[table_2_stats] unique # of drx (Single-Visit)\", unique_rx)\n",
    "\n",
    "    # # Multiple visit stats\n",
    "    multi_visit_sbj = work_table_1.loc[work_table_1 > 1].reset_index()[visit_record_key[0]] #SUBJECT_ID\n",
    "    print(\"\\n\\n[table_2_stats] # of patients (Multi-Visit)\", multi_visit_sbj.shape[0])\n",
    "    total_multi_visits = work_table_1.loc[work_table_1 > 1].sum()\n",
    "    print(\"[table_2_stats] avg. # of visits (Multi-Visit)\", total_multi_visits/multi_visit_sbj.shape[0])\n",
    "    work_table_5 = pd.merge(multi_visit_sbj, diagnosis_icd, on=[\"SUBJECT_ID\"]).dropna().drop_duplicates() #SUBJECT_ID, HADM_ID, ICD9_CODE\n",
    "    print(\"[table_2_stats] avg. # of dx per visit per patient (Multi-Visit)(This includes the first visit dx as well)\"\\\n",
    "        , work_table_5.shape[0]/total_multi_visits) # it increases because we are including both  \n",
    "    work_table_6 = pd.merge(work_table_3, multi_visit_sbj, on=[\"SUBJECT_ID\"])[\"ATC4\"]\n",
    "    total_multi_rx = work_table_6.shape[0]\n",
    "    print(\"[table_2_stats] avg. # of rx per visit per patient (Multi-Visit)(This includes the first visit rx as well)\"\\\n",
    "        , total_multi_rx/total_multi_visits) # it increases because we are including both \n",
    "    print(\"[table_2_stats] unique # of dx (Multi-Visit)\", work_table_5[\"ICD9_CODE\"].nunique())\n",
    "    print(\"[table_2_stats] unique # of rx (Multi-Visit)\", work_table_6.nunique())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a82d6eb3804a92444605d7a5aa99dc8a820299debc222bc81f709e34341d1d1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('illi_MS_DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
