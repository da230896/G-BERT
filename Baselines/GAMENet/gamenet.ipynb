{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "\n",
    "from Utils import Vocab, multi_label_metric\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_DATA_PATH = \"../../Data\"\n",
    "GLOBAL_MODELS_PATH = \"../../Models\"\n",
    "GAMENET_DATA_PATH = \"./Data\"\n",
    "MULTI_VISIT_TEMPORAL_PKL = \"multi_visit_temporal.pkl\"\n",
    "EHR_ADJ_PKL = 'ehr_adj_matrix.pkl'\n",
    "\n",
    "ATC4 = \"ATC4\"\n",
    "ICD9_CODE = \"ICD9_CODE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline\">GAMENet</span>\n",
    "\n",
    "The following GAMENet implementation references code from Homework 5. Adjustments were made in consideration to the GAMENet baseline implemented in the original G-BERT paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a utility function to build a 'Vocabulary' from medical codes: ICD-9 (diagnoses) codes and ATC4 (medication) codes. The function takes in data in the following format:\n",
    "\n",
    "data = [ patient1, patient2, patient3, ... ] \\\n",
    "patient = [ diagnoses_codes, medication_codes ] \\\n",
    "diagnoses_codes = [ diagnoses_visit1, diagnoses_visit2, diagnoses_visit3 ... ] \\\n",
    "medication_codes = [ medication_visit1, medication_visit2, medication_visit3 ... ] \\\n",
    "diagnoses_visit = [ code, code, code, ... ] \\\n",
    "medication_visit = [ code, code, code, ... ]\n",
    "\n",
    "diagnoses_codes = data[0][0][0] \\\n",
    "medication_codes = data[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_patient_data(data):\n",
    "    vocab = Vocab()\n",
    "    for patient in data:\n",
    "        for visit in patient:\n",
    "            vocab.add_sentence(visit)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utility function creates a list of patient records using MIMIC-III patient data, a diagnosis Vocab, and a medication Vocab as input. The data is formatted as explained above. The ICD-9 and ATC codes for each patient record are converted into indices defined in the Vocabs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idx_records(multi_visit_data, diag_vocab, med_vocab):\n",
    "    def build_admissions_codes(vocab, column):\n",
    "        admissions = []\n",
    "        for visit in column:\n",
    "            codes = []\n",
    "            \n",
    "            for code in visit:\n",
    "                codes.append(vocab.word2idx[code])\n",
    "            admissions.append(codes)\n",
    "        return admissions\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for index, row in multi_visit_data.iterrows():\n",
    "        patient = []\n",
    "        \n",
    "        admissions_diag_codes = build_admissions_codes(diag_vocab, row[ICD9_CODE])\n",
    "        admissions_med_codes = build_admissions_codes(med_vocab, row[ATC4])\n",
    "\n",
    "        patient.append(admissions_diag_codes)\n",
    "        patient.append(admissions_med_codes)\n",
    "\n",
    "        records.append(patient)\n",
    "\n",
    "    # for index, row in multi_visit_data.iterrows():\n",
    "    #     patient = []\n",
    "    #     admissions = []\n",
    "        \n",
    "    #     for visit in row[ICD9_CODE]:\n",
    "    #         diag_codes = []\n",
    "    #         for code in visit:\n",
    "    #             diag_codes.append(diag_vocab.word2idx[code])\n",
    "        \n",
    "    #         admissions.append(diag_codes)\n",
    "    #     patient.append(admissions)\n",
    "\n",
    "    #     admissions = []\n",
    "    #     for visit in row[ATC4]:\n",
    "    #         med_codes = []\n",
    "    #         for code in visit:\n",
    "    #             med_codes.append(med_vocab.word2idx[code])\n",
    "            \n",
    "    #         admissions.append(med_codes)\n",
    "    #     patient.append(admissions)\n",
    "\n",
    "    #     records.append(patient)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code builds the vocabulary (Vocab) for the diagnoses codes and the medications codes, as well as converts the patient records' codes to indices. The lengths of each Vocab is also defined here. They will be used later as parameters in the neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_visit_data = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_TEMPORAL_PKL))\n",
    "\n",
    "diag_vocab = build_vocab_from_patient_data(multi_visit_data[ICD9_CODE])\n",
    "med_vocab = build_vocab_from_patient_data(multi_visit_data[ATC4])\n",
    "\n",
    "diag_vocab_size = len(diag_vocab)\n",
    "med_vocab_size = len(med_vocab)\n",
    "\n",
    "records = create_idx_records(multi_visit_data, diag_vocab, med_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(N, records):\n",
    "    \n",
    "    adj_matrix = torch.zeros(N, N)\n",
    "\n",
    "    for patient in records:\n",
    "        med_set = patient[1]\n",
    "        for visit in med_set:\n",
    "            for i, code_i in enumerate(visit):\n",
    "                for j, code_j in enumerate(visit):\n",
    "                    if j <= i:\n",
    "                        continue\n",
    "\n",
    "                    adj_matrix[code_i, code_j] = 1\n",
    "                    adj_matrix[code_j, code_i] = 1\n",
    "                    \n",
    "    return adj_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_adj_path = os.path.join(GAMENET_DATA_PATH, EHR_ADJ_PKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below creates an adjacency matrix out of the medication codes and outputs them as a PKL file. Running create_adjacency_matrix method takes ~2 minutes. Only uncomment the code to generate a new PKL file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# N = len(med_vocab.word2idx)\n",
    "\n",
    "# adj_matrix = create_adjacency_matrix(N, records)\n",
    "# dill.dump(adj_matrix, open(ehr_adj_path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of EHR adjacency matrix:  torch.Size([385, 385])\n"
     ]
    }
   ],
   "source": [
    "ehr_adj = pd.read_pickle(ehr_adj_path)\n",
    "print(\"The shape of EHR adjacency matrix: \", ehr_adj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an RNN to learn patient representations from the EHR data. In the orignal GAMENet, separate RNNs were used to encode both diagnosis and procedure embeddings. For our purposes, to compare accurately with G-BERT, we only create embeddings for diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, codes):        \n",
    "        emb_list = []\n",
    "        for code in codes:\n",
    "            emb = self.embeddings(torch.tensor(code))\n",
    "            \n",
    "            # take the mean and make one sample per batch\n",
    "            emb_mean = emb.mean(dim=0).unsqueeze(dim=0)\n",
    "            emb_list.append(emb_mean)\n",
    "        \n",
    "        emb_seq = torch.cat(emb_list, dim=0).unsqueeze(dim=0)\n",
    "        result, _ = self.rnn(emb_seq)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.normal_(self.embeddings.weight)\n",
    "        for param in self.rnn.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            else:\n",
    "                torch.nn.init.normal_(param.data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct PatientQuery to output patient representations using the diagnoses embeddings given by the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientQuery(nn.Module):\n",
    "    def __init__(self, diag_vocab, emb_dim=16):\n",
    "        super(PatientQuery, self).__init__()\n",
    "\n",
    "        self.diag_rnn = RNN(diag_vocab, emb_dim)\n",
    "        self.linear = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=True)\n",
    "        \n",
    "    def forward(self, codes_diag,):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            codes_diag: [[diag codes for visit1], [diag codes for visit2], ...]\n",
    "                - e.g., [[0,1], [1,2]]  \n",
    "        output:\n",
    "            query embedding:\n",
    "                - size: (#visits, emd_dim)\n",
    "        \"\"\"\n",
    "        diag_emb = self.diag_rnn(codes_diag)\n",
    "        \n",
    "        result = self.linear(diag_emb)\n",
    "        result = result.squeeze(0)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GraphConvolution module constructs a simple GCN layer to learn improved embeddings on medication combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, vocab_med_size, adj, emb_dim=16):\n",
    "        super(GCN, self).__init__()\n",
    "        adj = self.normalize(adj + np.eye(adj.shape[0]).astype(np.float32))\n",
    "        self.adj = torch.FloatTensor(adj.float())\n",
    "                \n",
    "        self.gcn1 = GraphConvolution(in_features=vocab_med_size, out_features=emb_dim)\n",
    "        self.gcn2 = GraphConvolution(in_features=emb_dim, out_features=emb_dim)\n",
    "        \n",
    "        self.x = torch.eye(vocab_med_size)\n",
    "    \n",
    "    def forward(self):\n",
    "        x = self.gcn1(self.x, self.adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, self.adj)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def normalize(self, adj):\n",
    "        adj = adj / (adj @ np.ones(adj.shape) + 1e-8)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MemoryBank, DynamicMemory, and Facts are all a part of the graph-augmented memory module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_med_size, adj_ehr, emb_dim=16):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: Combine information from EHR graph and DDI graph\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use GCN for EHR graph using adj_ehr and emb_dim as output simension\n",
    "        self.gcn_ehr = GCN(vocab_med_size=vocab_med_size, adj=adj_ehr, emb_dim=emb_dim)\n",
    "        \n",
    "        # TO-DO: Check if this is needed. \n",
    "        # Learnable weight between adj_ddi and adj_ehr \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self):\n",
    "        # get ehr graph feature\n",
    "        info_ehr = self.gcn_ehr.forward()\n",
    "        \n",
    "        # get weighted information\n",
    "        info_comb = info_ehr * self.weight\n",
    "        \n",
    "        return info_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMemory(nn.Module):\n",
    "    def __init__(self, med_vocab_size):\n",
    "        super(DynamicMemory, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: generate a historical mapping: query embedding -> multi-hot medication vector\n",
    "        \"\"\"\n",
    "        self.med_vocab_size = med_vocab_size\n",
    "    \n",
    "    def forward(self, queries, codes_med):\n",
    "        \"\"\"\n",
    "        Input:  queries\n",
    "                    - this is the historical query embedding, given by PatientQuery Module\n",
    "                    - size: (#visits - 1, emb_dim), delete the current query\n",
    "                codes_med\n",
    "                    - this is the historical ground truth med vector\n",
    "                    - format: a list of length (#visits - 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        DM_key = queries\n",
    "\n",
    "        DM_value = np.zeros((queries.shape[0], self.med_vocab_size))\n",
    "        \n",
    "        # transform codes_med to multi-hot and filling DM_value row by row (visit by visit)\n",
    "        for visit_i in range(len(codes_med)):\n",
    "            for code_j in codes_med[visit_i]:                \n",
    "                DM_value[visit_i][code_j] = 1\n",
    "        \n",
    "        # use torch.FloatTensor for DM_value\n",
    "        DM_value = torch.FloatTensor(DM_value)\n",
    "        \n",
    "        return DM_key, DM_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fact1(nn.Module):\n",
    "    def __init__(self, queries):\n",
    "        super(Fact1, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: extract the final embedding from input queries, q^t\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "    \n",
    "    def forward(self):\n",
    "        # assign last embedding to result\n",
    "        result = self.queries[-1]\n",
    "\n",
    "        # final size: (1, emb_dim)\n",
    "        result = result.unsqueeze(0)\n",
    "        return result\n",
    "    \n",
    "\n",
    "class Fact2(nn.Module):\n",
    "    def __init__(self, query, MB):\n",
    "        super(Fact2, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: get attention information from MB, o^b_t\n",
    "        Input:\n",
    "            query\n",
    "                - this is the last embedding\n",
    "            MB\n",
    "                - is the memory bank\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "    \n",
    "    def forward(self):\n",
    "        attn_score = torch.mm(self.query, self.MB.t())\n",
    "        attn_matrix = F.softmax(attn_score, dim=1)\n",
    "        result = torch.mm(attn_matrix, self.MB)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "class Fact3(nn.Module):\n",
    "    def __init__(self, query, MB, DM_key, DM_value):\n",
    "        super(Fact3, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: similar to Fact2, get information from the DM, o_d^t\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "        self.DM_key = DM_key\n",
    "        self.DM_value = DM_value\n",
    "    \n",
    "    def forward(self):\n",
    "        attn = F.softmax(torch.mm(self.query, self.DM_key.t()), dim=1)\n",
    "        value = torch.mm(attn, self.DM_value)\n",
    "        out = torch.mm(value, self.MB)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutNet(nn.Module):\n",
    "    def __init__(self, vocab_med, emb_dim=16):\n",
    "        super(OutNet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: combine fact1, fact2, fact3 to do final prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=3 * emb_dim, out_features=2 * emb_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=2 * emb_dim, out_features=vocab_med, bias=True)\n",
    "        \n",
    "    def forward(self, fact1, fact2, fact3):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            fact1, fact2, fact3:\n",
    "                - three facts q^t, o^t_b, o^t_d\n",
    "                - each size: (1, emb_dim)\n",
    "        \"\"\"\n",
    "        memory_out = torch.cat((fact1, fact2, fact3), dim=1)\n",
    "        \n",
    "        result = self.fc1(memory_out)\n",
    "        result = F.relu(result)\n",
    "        result = self.fc2(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAMENet(nn.Module):\n",
    "    def __init__(self, diag_vocab_size, med_vocab_size, ehr_adj, emb_dim=128):\n",
    "        super(GAMENet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: integrate the whole model together\n",
    "        \"\"\"\n",
    "        self.patient = PatientQuery(diag_vocab_size, emb_dim)\n",
    "        self.memorybank = MemoryBank(med_vocab_size, ehr_adj, emb_dim)\n",
    "        self.dynamicmemory = DynamicMemory(med_vocab_size)\n",
    "        self.outnet = OutNet(med_vocab_size, emb_dim)\n",
    "        \n",
    "    def forward(self, codes_diag, codes_med):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            codes_diag\n",
    "                - a list of length #visits\n",
    "                - each element is also itself a list\n",
    "            codes_med\n",
    "                - a list of length #visits - 1\n",
    "                - each element is also itself a list\n",
    "        \"\"\"\n",
    "\n",
    "        queries = self.patient(codes_diag)\n",
    "           \n",
    "        MB = self.memorybank()\n",
    "        DM_key, DM_value = self.dynamicmemory(queries, codes_med)\n",
    "\n",
    "        fact1 = Fact1(queries)()\n",
    "        fact2 = Fact2(fact1, MB)()\n",
    "        fact3 = Fact3(fact1, MB, DM_key, DM_value)()\n",
    "\n",
    "        result = self.outnet(fact1, fact2, fact3)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAMENet(diag_vocab_size, med_vocab_size, ehr_adj)\n",
    "output = model([[0,1], [1,2], [4,5]], [[0,1], [2,5]])\n",
    "\n",
    "assert output.shape == (1, med_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Model Parameters:  961538\n"
     ]
    }
   ],
   "source": [
    "num_param = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of Model Parameters: \", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "tmp = records.copy()\n",
    "random.shuffle(tmp)\n",
    "\n",
    "# 67%: 16.5% 16.5% split training and test sets randomly\n",
    "records_size = len(records)\n",
    "\n",
    "train_end = int(records_size * 0.67)\n",
    "validation_end = int(records_size * 0.165) + train_end\n",
    "\n",
    "train_set = tmp[:train_end]\n",
    "validation_set = tmp[train_end:validation_end]\n",
    "test_set = tmp[validation_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..........................................................\n",
      "Current Epoch:  0\n",
      "===================\n",
      "Current Avg. Jaccard Score:  0.20377564452435226\n",
      "Current Avg. PR-AUC Score:  0.3166924289699309\n",
      "Current Avg. F1 Score:  0.33856083640049073\n",
      "Current Avg. Accuracy Score:  0.8077146733863152\n",
      "..................................................................\n",
      "Evaluating........................................................\n",
      "Evaluation Jaccard Score: 0.2024369234817539 \n",
      "Evaluation PR-AUC Score: 0.30647233454692896\n",
      "Evaluation F1 Score: 0.33671108983510145\n",
      "Evaluation Accuracy Score: 0.810655597690547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, jaccard_score, average_precision_score, f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "date_string = datetime.now().strftime(\"_%y%m%d_%H%M%S\")\n",
    "\n",
    "model = GAMENet(diag_vocab_size, med_vocab_size, ehr_adj)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def dataFormatter(patient_list):\n",
    "    diag_list, med_list = [], []\n",
    "    for diag in patient_list[0]:\n",
    "        diag_list.append(diag)\n",
    "\n",
    "    for med in patient_list[1]:\n",
    "        med_list.append(med)\n",
    "    \n",
    "    target = np.zeros((1, med_vocab_size))\n",
    "    target[0, med_list[-1]] = 1\n",
    "    return diag_list, med_list[:-1], torch.FloatTensor(target)\n",
    "\n",
    "def test(model, test_set):\n",
    "    model.eval()\n",
    "    pred_list, pred_prob_list, target_list = [], [], []\n",
    "    for patient in test_set:\n",
    "        for idx, visit in enumerate(patient[1]):\n",
    "            codes_diag, codes_med, target = dataFormatter(patient)\n",
    "            pred = model(codes_diag, codes_med).detach().cpu().numpy()[0]\n",
    "            pred_prob_list += pred.tolist()\n",
    "            pred[pred >= 0.5] = 1; pred[pred < 0.5] = 0\n",
    "            pred_list += pred.tolist(); target_list += target.numpy().tolist()[0]\n",
    "\n",
    "    return pred_list, target_list, pred_prob_list\n",
    "\n",
    "def train(train_set):\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        jaccard_list = []\n",
    "        pr_auc_list = []\n",
    "        f1_list = []\n",
    "        accuracy_list = []\n",
    "\n",
    "        for patient in train_set:\n",
    "            loss = 0\n",
    "\n",
    "            for idx, visit in enumerate(patient[1]):\n",
    "                codes_diag, codes_med, target = dataFormatter(patient)\n",
    "                # get the target of multilabel_margin_loss\n",
    "                multi_target = np.full((1, med_vocab_size), -1)\n",
    "                \n",
    "                for idx, item in enumerate(visit):\n",
    "                    multi_target[0][idx] = item\n",
    "                multi_target = torch.LongTensor(multi_target)\n",
    "\n",
    "                pred = model(codes_diag, codes_med)\n",
    "                \n",
    "                loss += F.binary_cross_entropy_with_logits(pred, target) + \\\n",
    "                                    F.multilabel_margin_loss(torch.sigmoid(pred), multi_target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # use the first 100 test patients to compute intermediate accuracy\n",
    "        pred_list, target_list, pred_prob_list = test(model, test_set[:100])\n",
    "\n",
    "        # _, current_pr_auc, _ = multi_label_metric(target_list, pred_list, pred_prob_list)\n",
    "        current_jaccard = jaccard_score(target_list, pred_list)\n",
    "        current_pr_auc = average_precision_score(target_list, pred_prob_list)\n",
    "        current_f1 = f1_score(target_list, pred_list)\n",
    "        current_acc = accuracy_score(pred_list, target_list)\n",
    "\n",
    "        jaccard_list.append(current_jaccard)\n",
    "        pr_auc_list.append(current_pr_auc)\n",
    "        f1_list.append(current_f1)\n",
    "        accuracy_list.append(current_acc)\n",
    "        \n",
    "        print(\"Current Epoch: \", i)\n",
    "        print(\"===================\")\n",
    "        print(\"Current Avg. Jaccard Score: \", sum(jaccard_list) / len(jaccard_list))\n",
    "        print(\"Current Avg. PR-AUC Score: \", sum(pr_auc_list) / len(pr_auc_list))\n",
    "        print(\"Current Avg. F1 Score: \", sum(f1_list) / len(f1_list))\n",
    "        print(\"Current Avg. Accuracy Score: \", sum(accuracy_list) / len(accuracy_list))\n",
    "        print(\"..................................................................\")\n",
    "\n",
    "    torch.save({\n",
    "            \"model_state_dict\": model.state_dict()\n",
    "            }, os.path.join(GLOBAL_MODELS_PATH,\"gamenet\" + date_string))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Training..........................................................\")\n",
    "eval_model = train(train_set)\n",
    "checkpoint = torch.load(os.path.join(GLOBAL_MODELS_PATH, \"gamenet\" + date_string))\n",
    "        \n",
    "print(\"Evaluating........................................................\")\n",
    "pred_list, target_list, pred_prob_list = test(eval_model, test_set)\n",
    "evaluation_jaccard_score = \"Evaluation Jaccard Score: {0} \".format(jaccard_score(target_list, pred_list))\n",
    "evaluation_pr_auc_score = \"Evaluation PR-AUC Score: {0}\".format(average_precision_score(target_list, pred_prob_list, average='macro'))\n",
    "evaluation_f1_score = \"Evaluation F1 Score: {0}\".format(f1_score(target_list, pred_list))\n",
    "evaluation_accuracy_score = \"Evaluation Accuracy Score: {0}\".format(accuracy_score(target_list, pred_list))\n",
    "\n",
    "print(evaluation_jaccard_score)\n",
    "print(evaluation_pr_auc_score)\n",
    "print(evaluation_f1_score)\n",
    "print(evaluation_accuracy_score)\n",
    "\n",
    "results_file = os.path.join(\"Results\", \"results\" + date_string + \".txt\")\n",
    "file_obj = open(results_file, 'w+')\n",
    "file_obj.write(\"Number of Epochs: \" + str(EPOCHS) + \"\\n\")\n",
    "file_obj.write(\"Learning Rate: \" + str(LEARNING_RATE) + \"\\n\")\n",
    "file_obj.write(\"Number of Parameters: \" + str(num_param) + \"\\n\")\n",
    "file_obj.write(evaluation_jaccard_score + \"\\n\")\n",
    "file_obj.write(evaluation_pr_auc_score + \"\\n\")\n",
    "file_obj.write(evaluation_f1_score + \"\\n\")\n",
    "file_obj.write(evaluation_accuracy_score + \"\\n\")\n",
    "\n",
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of GAMENet Training\n",
    "===============================\n",
    "*** Stats *** \\\n",
    "Number of Epochs: 75 \\\n",
    "Learning Rate: 0.001 \\\n",
    "Number of Parameters: 961538 \n",
    "\n",
    "Average Jaccard Score: 0.2633452915764412 \\\n",
    "Average PR-AUC Score: 0.32996452114879504 \\\n",
    "Average F1 Score: 0.4169015285565054 \\\n",
    "Average Accuracy Score: 0.8665114667369459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
