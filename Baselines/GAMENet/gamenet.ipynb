{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "\n",
    "from Utils import Vocab\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_DATA_PATH = \"../../Data\"\n",
    "GAMENET_DATA_PATH = \"./Data\"\n",
    "MULTI_VISIT_TEMPORAL_PKL = \"multi_visit_temporal.pkl\"\n",
    "EHR_ADJ_PKL = 'ehr_adj_matrix.pkl'\n",
    "\n",
    "ATC4 = \"ATC4\"\n",
    "ICD9_CODE = \"ICD9_CODE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline\">GAMENet</span>\n",
    "\n",
    "The following GAMENet implementation references code from Homework 5. Adjustments were made in consideration to the GAMENet baseline implemented in the original G-BERT paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a utility function to build a 'Vocabulary' from medical codes: ICD-9 (diagnoses) codes and ATC4 (medication) codes. The function takes in data in the following format:\n",
    "\n",
    "data = [ patient1, patient2, patient3, ... ] \\\n",
    "patient = [ diagnoses_codes, medication_codes ] \\\n",
    "diagnoses_codes = [ diagnoses_visit1, diagnoses_visit2, diagnoses_visit3 ... ] \\\n",
    "medication_codes = [ medication_visit1, medication_visit2, medication_visit3 ... ] \\\n",
    "diagnoses_visit = [ code, code, code, ... ] \\\n",
    "medication_visit = [ code, code, code, ... ]\n",
    "\n",
    "diagnoses_codes = data[0][0][0] \\\n",
    "medication_codes = data[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_patient_data(data):\n",
    "    vocab = Vocab()\n",
    "    for patient in data:\n",
    "        for visit in patient:\n",
    "            vocab.add_sentence(visit)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utility function creates a list of patient records using MIMIC-III patient data, a diagnosis Vocab, and a medication Vocab as input. The data is formatted as explained above. The ICD-9 and ATC codes for each patient record are converted into indices defined in the Vocabs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idx_records(multi_visit_data, diag_vocab, med_vocab):\n",
    "    records = []\n",
    "    for index, row in multi_visit_data.iterrows():\n",
    "        patient = []\n",
    "        admissions = []\n",
    "        \n",
    "        for visit in row[ICD9_CODE]:\n",
    "            diag_codes = []\n",
    "            for code in visit:\n",
    "                diag_codes.append(diag_vocab.word2idx[code])\n",
    "        \n",
    "            admissions.append(diag_codes)\n",
    "        patient.append(admissions)\n",
    "\n",
    "        admissions = []\n",
    "        for visit in row[ATC4]:\n",
    "            med_codes = []\n",
    "            for code in visit:\n",
    "                med_codes.append(med_vocab.word2idx[code])\n",
    "            \n",
    "            admissions.append(med_codes)\n",
    "        patient.append(admissions)\n",
    "\n",
    "        records.append(patient)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code builds the diagnoses Vocab, the medication Vocab, and the patient records as codes converted to indices. The lengths of each Vocab is also defined here. They will be used as parameters in the neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_visit_data = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_TEMPORAL_PKL))\n",
    "\n",
    "diag_vocab = build_vocab_from_patient_data(multi_visit_data[ICD9_CODE])\n",
    "med_vocab = build_vocab_from_patient_data(multi_visit_data[ATC4])\n",
    "\n",
    "diag_vocab_size = len(diag_vocab)\n",
    "med_vocab_size = len(med_vocab)\n",
    "\n",
    "records = create_idx_records(multi_visit_data, diag_vocab, med_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(N, records):\n",
    "    \n",
    "    adj_matrix = torch.zeros(N, N)\n",
    "\n",
    "    for patient in records:\n",
    "        med_set = patient[1]\n",
    "        for visit in med_set:\n",
    "            for i, code_i in enumerate(visit):\n",
    "                for j, code_j in enumerate(visit):\n",
    "                    if j <= i:\n",
    "                        continue\n",
    "\n",
    "                    adj_matrix[code_i, code_j] = 1\n",
    "                    adj_matrix[code_j, code_i] = 1\n",
    "                    \n",
    "    return adj_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_adj_path = os.path.join(GAMENET_DATA_PATH, EHR_ADJ_PKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below creates an adjacency matrix out of the medication codes and outputs them as a PKL file. Running create_adjacency_matrix method takes ~2 minutes. Only uncomment the code to generate a new PKL file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# N = len(med_vocab.word2idx)\n",
    "\n",
    "# adj_matrix = create_adjacency_matrix(N, records)\n",
    "# dill.dump(adj_matrix, open(ehr_adj_path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of EHR adjacency matrix:  torch.Size([385, 385])\n"
     ]
    }
   ],
   "source": [
    "ehr_adj = pd.read_pickle(ehr_adj_path)\n",
    "print(\"The shape of EHR adjacency matrix: \", ehr_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, codes):        \n",
    "        emb_list = []\n",
    "        for code in codes:\n",
    "            emb = None\n",
    "            emb = self.embeddings(torch.tensor(code))\n",
    "            \n",
    "            # take the mean and make one sample per batch\n",
    "            emb_mean = emb.mean(dim=0).unsqueeze(dim=0)\n",
    "            emb_list.append(emb_mean)\n",
    "        \n",
    "        # Check emb_list, inform Dinesh\n",
    "        emb_seq = torch.cat(emb_list, dim=0).unsqueeze(dim=0)\n",
    "        #print(\"Embedding Sequence: \")\n",
    "        #print(emb_seq)\n",
    "        result, _ = self.rnn(emb_seq)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.normal_(self.embeddings.weight)\n",
    "        for param in self.rnn.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            else:\n",
    "                torch.nn.init.normal_(param.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientQuery(nn.Module):\n",
    "    def __init__(self, diag_vocab, emb_dim=16):\n",
    "        super(PatientQuery, self).__init__()\n",
    "\n",
    "        self.diag_rnn = RNN(diag_vocab, emb_dim)\n",
    "        self.linear = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=True)\n",
    "        \n",
    "    def forward(self, codes_diag,):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            codes_diag: [[diag codes for visit1], [diag codes for visit2], ...]\n",
    "                - e.g., [[0,1], [1,2]]  \n",
    "        output:\n",
    "            query embedding:\n",
    "                - size: (#visits, emd_dim)\n",
    "        \"\"\"\n",
    "        # get diag and prod embedding by self.rnn_diag, self.rnn_prod\n",
    "        diag_emb = self.diag_rnn(codes_diag)\n",
    "        \n",
    "        # concat emb_diag and emb_prod, then tranfrom by self.linear\n",
    "        result = self.linear(diag_emb)\n",
    "        result = result.squeeze(0)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, vocab_med_size, adj, emb_dim=16):\n",
    "        super(GCN, self).__init__()\n",
    "        adj = self.normalize(adj + np.eye(adj.shape[0]).astype(np.float32))\n",
    "        self.adj = torch.FloatTensor(adj.float())\n",
    "        \n",
    "        # self.gcn1 = GCNConv(in_channels=vocab_med_size, out_channels=emb_dim)\n",
    "        # self.gcn2 = GCNConv(in_channels=emb_dim, out_channels=emb_dim)\n",
    "        \n",
    "        self.gcn1 = GraphConvolution(in_features=vocab_med_size, out_features=emb_dim)\n",
    "        self.gcn2 = GraphConvolution(in_features=emb_dim, out_features=emb_dim)\n",
    "        \n",
    "        # the initial feature\n",
    "        self.x = torch.eye(vocab_med_size)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1 (already done). use self.gcn1 for the first graph convolution\n",
    "                - remember to use self.adj\n",
    "            2. use F.relu() as the activation function\n",
    "            3. use self.gcn2\n",
    "        \"\"\"\n",
    "        x = self.gcn1(self.x, self.adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, self.adj)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def normalize(self, adj):\n",
    "        adj = adj / (adj @ np.ones(adj.shape) + 1e-8)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_med_size, adj_ehr, emb_dim=16):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: Combine information from EHR graph and DDI graph\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use GCN for EHR graph using adj_ehr and emb_dim as output simension\n",
    "        self.gcn_ehr = GCN(vocab_med_size=vocab_med_size, adj=adj_ehr, emb_dim=emb_dim)\n",
    "        \n",
    "        # TO-DO: Check if this is needed. \n",
    "        # Learnable weight between adj_ddi and adj_ehr \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self):\n",
    "        # get ehr graph feature\n",
    "        info_ehr = self.gcn_ehr.forward()\n",
    "        \n",
    "        # get weighted information\n",
    "        info_comb = info_ehr * self.weight\n",
    "        \n",
    "        return info_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMemory(nn.Module):\n",
    "    def __init__(self, med_vocab_size):\n",
    "        super(DynamicMemory, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: generate a historical mapping: query embedding -> multi-hot medication vector\n",
    "        \"\"\"\n",
    "        self.med_vocab_size = med_vocab_size\n",
    "    \n",
    "    def forward(self, queries, codes_med):\n",
    "        \"\"\"\n",
    "        Input:  queries\n",
    "                    - this is the historical query embedding, given by PatientQuery Module\n",
    "                    - size: (#visits - 1, emb_dim), delete the current query\n",
    "                codes_med\n",
    "                    - this is the historical groud truth med vector\n",
    "                    - format: a list of length (#visits - 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        DM_key = queries\n",
    "\n",
    "        DM_value = np.zeros((queries.shape[0], self.med_vocab_size))\n",
    "        \n",
    "        # transform codes_med to multi-hot and filling DM_value row by row (visit by visit)\n",
    "        for visit_i in range(len(codes_med)):\n",
    "            for code_j in codes_med[visit_i]:                \n",
    "                DM_value[visit_i][code_j] = 1\n",
    "        \n",
    "        # use torch.FloatTensor for DM_value\n",
    "        DM_value = torch.FloatTensor(DM_value)\n",
    "        \n",
    "        return DM_key, DM_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fact1(nn.Module):\n",
    "    def __init__(self, queries):\n",
    "        super(Fact1, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: extract the final embedding from input queries, q^t\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "    \n",
    "    def forward(self):\n",
    "        # assign last embedding to result\n",
    "        result = self.queries[-1]\n",
    "\n",
    "        # final size: (1, emb_dim)\n",
    "        result = result.unsqueeze(0)\n",
    "        return result\n",
    "    \n",
    "\n",
    "class Fact2(nn.Module):\n",
    "    def __init__(self, query, MB):\n",
    "        super(Fact2, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: get attention information from MB, o^b_t\n",
    "        Input:\n",
    "            query\n",
    "                - this is the last embedding\n",
    "            MB\n",
    "                - is the memory bank\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "    \n",
    "    def forward(self):\n",
    "        # get the attention weight between query and each row of MB, adding to attn_score\n",
    "        # need to transpose MB, using MB.t()\n",
    "        attn_score = torch.mm(self.query, self.MB.t())\n",
    "        # use F.softmax(, dim=1) to compute the attention matrix, attn_matrix\n",
    "        attn_matrix = F.softmax(attn_score, dim=1)\n",
    "        # get the final result from attn_matrix and MB\n",
    "        result = torch.mm(attn_matrix, self.MB)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "class Fact3(nn.Module):\n",
    "    def __init__(self, query, MB, DM_key, DM_value):\n",
    "        super(Fact3, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: similar to Fact2, get information from the DM, o_d^t\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "        self.DM_key = DM_key\n",
    "        self.DM_value = DM_value\n",
    "    \n",
    "    def forward(self):\n",
    "        attn = F.softmax(torch.mm(self.query, self.DM_key.t()), dim=1)\n",
    "        value = torch.mm(attn, self.DM_value)\n",
    "        out = torch.mm(value, self.MB)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutNet(nn.Module):\n",
    "    def __init__(self, vocab_med, emb_dim=16):\n",
    "        super(OutNet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: combine fact1, fact2, fact3 to do final prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. build the first linear layer\n",
    "                - input size: 3 * emb_dim\n",
    "                - end size: 2 * emb_dim\n",
    "                - use bias=True option\n",
    "            2. build the second linear layer\n",
    "                - input size: 2 * emb_dim\n",
    "                - end size: vocab_med\n",
    "                - use bias=True option\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(in_features=3 * emb_dim, out_features=2 * emb_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=2 * emb_dim, out_features=vocab_med, bias=True)\n",
    "        \n",
    "    def forward(self, fact1, fact2, fact3):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            fact1, fact2, fact3:\n",
    "                - three facts q^t, o^t_b, o^t_d\n",
    "                - each size: (1, emb_dim)\n",
    "        \"\"\"\n",
    "        # concat fact1, fact2, fact3, and assign to memory_out\n",
    "        # size: (1, 3 * emb_dim)\n",
    "        memory_out = torch.cat((fact1, fact2, fact3), dim=1)\n",
    "        \n",
    "        result = self.fc1(memory_out)\n",
    "        result = F.relu(result)\n",
    "        result = self.fc2(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAMENet(nn.Module):\n",
    "    def __init__(self, diag_vocab_size, med_vocab_size, ehr_adj, emb_dim=128):\n",
    "        super(GAMENet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: integrate the whole model together\n",
    "        \"\"\"\n",
    "        self.patient = PatientQuery(diag_vocab_size, emb_dim)\n",
    "        self.memorybank = MemoryBank(med_vocab_size, ehr_adj, emb_dim)\n",
    "        self.dynamicmemory = DynamicMemory(med_vocab_size)\n",
    "        self.outnet = OutNet(med_vocab_size, emb_dim)\n",
    "        \n",
    "    def forward(self, codes_diag, codes_med):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            codes_diag\n",
    "                - a list of length #visits\n",
    "                - each element is also itself a list\n",
    "            codes_med\n",
    "                - a list of length #visits - 1\n",
    "                - each element is also itself a list\n",
    "        \"\"\"\n",
    "        \n",
    "        # get patient query embedding (#visit, emb_dim*2)\n",
    "        # get patient query embedding (#visit, emb_dim) ??\n",
    "        queries = self.patient(codes_diag)\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. build a memory bank, assign to MB\n",
    "                - use self.memorybank\n",
    "            2. build a dynamic memory, assign to DM_key and DM_value\n",
    "                - use self.dynamicmemory\n",
    "                - use queries and codes_med as features\n",
    "        \"\"\"\n",
    "        MB = None\n",
    "        DM_key, DM_value = None, None\n",
    "           \n",
    "        MB = self.memorybank()\n",
    "        DM_key, DM_value = self.dynamicmemory(queries, codes_med)\n",
    "\n",
    "        # extract three memory outputs, assign to fact1, fact2, fact3\n",
    "        fact1 = Fact1(queries)()\n",
    "        fact2 = Fact2(fact1, MB)()\n",
    "        fact3 = Fact3(fact1, MB, DM_key, DM_value)()\n",
    "\n",
    "        # get the final output\n",
    "        result = self.outnet(fact1, fact2, fact3)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAMENet(diag_vocab_size, med_vocab_size, ehr_adj)\n",
    "output = model([[0,1], [1,2], [4,5]], [[0,1], [2,5]])\n",
    "\n",
    "assert output.shape == (1, med_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Model Parameters:  961538\n"
     ]
    }
   ],
   "source": [
    "num_param = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of Model Parameters: \", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "tmp = records.copy()\n",
    "random.shuffle(tmp)\n",
    "\n",
    "# 67%: 16.5% 16.5% split training and test sets randomly\n",
    "records_size = len(records)\n",
    "\n",
    "train_end = int(records_size * 0.67)\n",
    "validation_end = int(records_size * 0.165) + train_end\n",
    "\n",
    "train_set = tmp[:train_end]\n",
    "validation_set = tmp[train_end:validation_end]\n",
    "test_set = tmp[validation_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters\n",
      "961538\n"
     ]
    }
   ],
   "source": [
    "model = GAMENet(diag_vocab_size, med_vocab_size, ehr_adj)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def dataFormatter(patient_list):\n",
    "\n",
    "    diag_list, med_list = [], []\n",
    "    for diag in patient_list[0]:\n",
    "        diag_list.append(diag)\n",
    "\n",
    "    for med in patient_list[1]:\n",
    "        med_list.append(med)\n",
    "    \n",
    "    target = np.zeros((1, med_vocab_size))\n",
    "    target[0, med_list[-1]] = 1\n",
    "    return diag_list, med_list[:-1], torch.FloatTensor(target)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(test_set):\n",
    "    model.eval()\n",
    "    pred_list, target_list = [], []\n",
    "    for patient in test_set:\n",
    "        for idx, visit in enumerate(patient[1]):\n",
    "            codes_diag, codes_med, target = dataFormatter(patient)\n",
    "            pred = model(codes_diag, codes_med).detach().cpu().numpy()[0]\n",
    "            pred[pred >= 0.5] = 1; pred[pred < 0.5] = 0\n",
    "            pred_list += pred.tolist(); target_list += target.numpy().tolist()[0]\n",
    "    return pred_list, target_list\n",
    "\n",
    "def train(train_set):\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        for patient in train_set:\n",
    "            loss = 0\n",
    "\n",
    "            for idx, visit in enumerate(patient[1]):\n",
    "                codes_diag, codes_med, target = dataFormatter(patient)\n",
    "                # get the target of multilabel_margin_loss\n",
    "                multi_target = np.full((1, med_vocab_size), -1)\n",
    "                \n",
    "                for idx, item in enumerate(visit):\n",
    "                    multi_target[0][idx] = item\n",
    "                multi_target = torch.LongTensor(multi_target)\n",
    "\n",
    "                pred = model(codes_diag, codes_med)\n",
    "                \n",
    "                loss += F.binary_cross_entropy_with_logits(pred, target) + \\\n",
    "                                    F.multilabel_margin_loss(torch.sigmoid(pred), multi_target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # use the first 100 test patients to compute intermediate accuracy\n",
    "        print(accuracy_score(*test(test_set[:100])))\n",
    "    \n",
    "train(train_set)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
