{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "\n",
    "from Globals import UNIQUE_ATC_CSV, UNIQUE_ICD_CSV\n",
    "from Utils import Vocab\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch_geometric.nn import Sequential, GCNConv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_DATA_PATH = \"../../Data\"\n",
    "ATC4 = \"ATC4\"\n",
    "ICD9_CODE = \"ICD9_CODE\"\n",
    "MULTI_VISIT_TEMPORAL_PKL = \"multi_visit_temporal.pkl\"\n",
    "GAMENET_DATA_PATH = \"./Data\"\n",
    "EHR_ADJ_PKL = 'ehr_adj_matrix.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_patient_data(data):\n",
    "    vocab = Vocab()\n",
    "    for patient in data:\n",
    "        for visit in patient:\n",
    "            vocab.add_sentence(visit)\n",
    "    \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocab of ATC (medication) and ICD-9 (diagnoses) codes for each patient\n",
    "\n",
    "multi_visit_records = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_TEMPORAL_PKL))\n",
    "\n",
    "diag_vocab = build_vocab_from_patient_data(multi_visit_records[ICD9_CODE])\n",
    "med_vocab = build_vocab_from_patient_data(multi_visit_records[ATC4])\n",
    "\n",
    "diag_vocab_size = len(diag_vocab)\n",
    "med_vocab_size = len(med_vocab)\n",
    "\n",
    "records = []\n",
    "for index, row in multi_visit_records.iterrows():\n",
    "    patient = []\n",
    "    admissions = []\n",
    "    \n",
    "    for visit in row[ICD9_CODE]:\n",
    "        diag_codes = []\n",
    "        for code in visit:\n",
    "            diag_codes.append(diag_vocab.word2idx[code])\n",
    "    \n",
    "        admissions.append(diag_codes)\n",
    "    patient.append(admissions)\n",
    "\n",
    "    admissions = []\n",
    "    for visit in row[ATC4]:\n",
    "        med_codes = []\n",
    "        for code in visit:\n",
    "            med_codes.append(med_vocab.word2idx[code])\n",
    "        \n",
    "        admissions.append(med_codes)\n",
    "    patient.append(admissions)\n",
    "\n",
    "    records.append(patient)\n",
    "\n",
    "# Format of records:\n",
    "# Patient: [[diag_code, diag_code, diag_code], [med_code, med_code, med_code]]\n",
    "# for row in multi_visit_records[ICD9_CODE]:\n",
    "#     admission = []\n",
    "#     patient = []\n",
    "\n",
    "#     for visit in row:\n",
    "#         admission.append([diag_vocab.word2idx[i] for i in visit])\n",
    "    # admission.append([med_vocab.word2idx[i] for i in row[ATC4]])\n",
    "    # patient.append(admission)\n",
    "    # print(patient)\n",
    "    # records.append(patient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create adjacency matrix from medication codes\n",
    "def create_adjacency_matrix(N, records):\n",
    "    \n",
    "    adj_matrix = torch.zeros(N, N)\n",
    "\n",
    "    for patient in records:\n",
    "        med_set = patient[1]\n",
    "        #print(med_set)\n",
    "        for visit in med_set:\n",
    "            for i, code_i in enumerate(visit):\n",
    "                for j, code_j in enumerate(visit):\n",
    "                    if j <= i:\n",
    "                        continue\n",
    "\n",
    "                    adj_matrix[code_i, code_j] = 1\n",
    "                    adj_matrix[code_j, code_i] = 1\n",
    "                    \n",
    "    return adj_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_adj_path = os.path.join(GAMENET_DATA_PATH, EHR_ADJ_PKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below to create adjacency matrix and output as PKL file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "\n",
    "# N = len(med_vocab.word2idx)\n",
    "\n",
    "# adj_matrix = create_adjacency_matrix(N, records)\n",
    "# dill.dump(adj_matrix, open(ehr_adj_path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "ehr_adj = pd.read_pickle(ehr_adj_path)\n",
    "# print(ehr_adj)\n",
    "print(ehr_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, codes):        \n",
    "        emb_list = []\n",
    "        for code in codes:\n",
    "            emb = None\n",
    "            emb = self.embeddings(torch.tensor(code))\n",
    "            \n",
    "            # take the mean and make one sample per batch\n",
    "            emb_mean = emb.mean(dim=0).unsqueeze(dim=0)\n",
    "            emb_list.append(emb_mean)\n",
    "            \n",
    "        emb_seq = torch.cat(emb_list, dim=0).unsqueeze(dim=0)\n",
    "        \n",
    "        result, _ = self.rnn(emb_seq)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.normal_(self.embeddings.weight)\n",
    "        for param in self.rnn.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            else:\n",
    "                torch.nn.init.normal_(param.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientQuery(nn.Module):\n",
    "    def __init__(self, diag_vocab, emb_dim=16):\n",
    "        super(PatientQuery, self).__init__()\n",
    "\n",
    "        self.diag_rnn = RNN(diag_vocab, emb_dim)\n",
    "        # self.linear = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=True)\n",
    "        self.linear = nn.Linear(in_features=emb_dim, out_features=emb_dim, bias=True)\n",
    "        \n",
    "    def forward(self, codes_diag,):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            codes_diag: [[diag codes for visit1], [diag codes for visit2], ...]\n",
    "                - e.g., [[0,1], [1,2]]  \n",
    "            codes_prod: [[prod codes for visit1], [prod codes for visit2], ...]\n",
    "                - e.g., [[0,1], [1,2]]  \n",
    "        output:\n",
    "            query embedding:\n",
    "                - size: (#visits, emd_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: get diag and prod embedding by self.rnn_diag, self.rnn_prod\n",
    "        \"\"\"\n",
    "        diag_emb = self.diag_rnn(codes_diag)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: concat emb_diag and emb_prod, then tranfrom by self.linear\n",
    "        \"\"\"\n",
    "        result = self.linear(diag_emb)\n",
    "        result = result.squeeze(0)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, vocab_med_size, adj, emb_dim=16):\n",
    "        super(GCN, self).__init__()\n",
    "        adj = self.normalize(adj + np.eye(adj.shape[0]).astype(np.float32))\n",
    "        self.adj = torch.FloatTensor(adj.float())\n",
    "        \n",
    "        # self.gcn1 = GCNConv(in_channels=vocab_med_size, out_channels=emb_dim)\n",
    "        # self.gcn2 = GCNConv(in_channels=emb_dim, out_channels=emb_dim)\n",
    "        \n",
    "        self.gcn1 = GraphConvolution(in_features=vocab_med_size, out_features=emb_dim)\n",
    "        self.gcn2 = GraphConvolution(in_features=emb_dim, out_features=emb_dim)\n",
    "        \n",
    "        # the initial feature\n",
    "        self.x = torch.eye(vocab_med_size)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1 (already done). use self.gcn1 for the first graph convolution\n",
    "                - remember to use self.adj\n",
    "            2. use F.relu() as the activation function\n",
    "            3. use self.gcn2\n",
    "        \"\"\"\n",
    "        x = self.gcn1(self.x, self.adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, self.adj)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def normalize(self, adj):\n",
    "        adj = adj / (adj @ np.ones(adj.shape) + 1e-8)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_med_size, adj_ehr, emb_dim=16):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: conbime information from EHR graph and DDI graph\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. use GCN for EHR graph\n",
    "                - using adj_ehr\n",
    "                - using emb_dim as the ouput dimension\n",
    "            2. use GCN for DDI graph\n",
    "                - using adj_ddi\n",
    "                - using emb_dim as the ouput dimension\n",
    "            3 (already done). design a learnable weight between them\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gcn_ehr = GCN(vocab_med_size=vocab_med_size, adj=adj_ehr, emb_dim=emb_dim)\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. get ehr graph feature\n",
    "                - using .forward() function\n",
    "            2. get ddi graph feature\n",
    "                - using .forward() function\n",
    "            3 (already done). get weighted information\n",
    "        \"\"\"\n",
    "        \n",
    "        info_ehr = self.gcn_ehr.forward()\n",
    "        \n",
    "        info_comb = info_ehr * self.weight\n",
    "        \n",
    "        return info_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMemory(nn.Module):\n",
    "    def __init__(self, med_vocab_size):\n",
    "        super(DynamicMemory, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: generate a historical mapping: query embedding -> multi-hot medication vector\n",
    "        \"\"\"\n",
    "        self.med_vocab_size = med_vocab_size\n",
    "    \n",
    "    def forward(self, queries, codes_med):\n",
    "        \"\"\"\n",
    "        Input:  queries\n",
    "                    - this is the historical query embedding, given by PatientQuery Module\n",
    "                    - size: (#visits - 1, emb_dim), delete the current query\n",
    "                codes_med\n",
    "                    - this is the historical groud truth med vector\n",
    "                    - format: a list of length (#visits - 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        DM_key = queries\n",
    "\n",
    "        DM_value = np.zeros((queries.shape[0], self.med_vocab_size))\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: transform codes_med to multi-hot and filling DM_value row by row (visit by visit)\n",
    "        \"\"\"\n",
    "        #print(DM_key)\n",
    "        #print(codes_med)\n",
    "        #print(DM_value)\n",
    "        #print(self.med_vocab_size)\n",
    "        #print(codes_med)\n",
    "        for visit_i in range(len(codes_med)):\n",
    "            for code_j in codes_med[visit_i]:                \n",
    "                DM_value[visit_i][code_j] = 1\n",
    "        \n",
    "        # use torch.FloatTensor for DM_value\n",
    "        DM_value = torch.FloatTensor(DM_value)\n",
    "        \n",
    "        return DM_key, DM_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fact1(nn.Module):\n",
    "    def __init__(self, queries):\n",
    "        super(Fact1, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: extract the final embedding from input queries, q^t\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: assign the last embedding to result\n",
    "                - final size: (1, emb_dim)\n",
    "                - make sure the output size is **not** a vector format (emb_dim,)\n",
    "        \"\"\"\n",
    "        result = self.queries[-1]\n",
    "\n",
    "        result = result.unsqueeze(0)\n",
    "        return result\n",
    "    \n",
    "\n",
    "class Fact2(nn.Module):\n",
    "    def __init__(self, query, MB):\n",
    "        super(Fact2, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: get attention information from MB, o^t_b\n",
    "        Input:\n",
    "            query\n",
    "                - this is the last embedding\n",
    "            MB\n",
    "                - is the memory bank\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. get the attention weight between query and each row of MB, adding to attn_score\n",
    "                - use torch.mm()\n",
    "                - tip: need to transpose MB, using MB.t()\n",
    "            2. use F.softmax(, dim=1) to compute the attention matrix, attn_matrix\n",
    "            3. get the final result from attn_matrix and MB\n",
    "                - use torch.mm(,)\n",
    "        \"\"\"\n",
    "        attn_score = torch.mm(self.query, self.MB.t())\n",
    "        attn_matrix = F.softmax(attn_score, dim=1)\n",
    "        result = torch.mm(attn_matrix, self.MB)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "class Fact3(nn.Module):\n",
    "    def __init__(self, query, MB, DM_key, DM_value):\n",
    "        super(Fact3, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: similar to Fact2, get information from the DM, o_d^t\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "        self.DM_key = DM_key\n",
    "        self.DM_value = DM_value\n",
    "    \n",
    "    def forward(self):\n",
    "        attn = F.softmax(torch.mm(self.query, self.DM_key.t()), dim=1)\n",
    "        value = torch.mm(attn, self.DM_value)\n",
    "        out = torch.mm(value, self.MB)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutNet(nn.Module):\n",
    "    def __init__(self, vocab_med, emb_dim=16):\n",
    "        super(OutNet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: combine fact1, fact2, fact3 to do final prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. build the first linear layer\n",
    "                - input size: 3 * emb_dim\n",
    "                - end size: 2 * emb_dim\n",
    "                - use bias=True option\n",
    "            2. build the second linear layer\n",
    "                - input size: 2 * emb_dim\n",
    "                - end size: vocab_med\n",
    "                - use bias=True option\n",
    "        \"\"\"\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=3 * emb_dim, out_features=2 * emb_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=2 * emb_dim, out_features=vocab_med, bias=True)\n",
    "        \n",
    "    def forward(self, fact1, fact2, fact3):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            fact1, fact2, fact3:\n",
    "                - three facts q^t, o^t_b, o^t_d\n",
    "                - each size: (1, emb_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. concat fact1, fact2, fact3, and assign to memory_out\n",
    "                - size: (1, 3 * emb_dim)\n",
    "            2. pass through the first linear layer\n",
    "            3. use F.relu() as the activation\n",
    "            3. pass through the second linear layer\n",
    "        \"\"\"\n",
    "        memory_out = None\n",
    "        result = None\n",
    "        \n",
    "        memory_out = torch.cat((fact1, fact2, fact3), dim=1)\n",
    "        #print(memory_out.size())\n",
    "        \n",
    "        result = self.fc1(memory_out)\n",
    "        result = F.relu(result)\n",
    "        result = self.fc2(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAMENet(nn.Module):\n",
    "    def __init__(self, diag_vocab_size, med_vocab_size, ehr_adj, emb_dim=128):\n",
    "        super(GAMENet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: integrate the whole model together\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. build a patient query network\n",
    "                - PatientQuery\n",
    "                - use diag_vocab_size, vocab_prod, emb_dim\n",
    "            2. build a memory bank\n",
    "                - MemoryBank\n",
    "                - use vocab_med, adj_ehr, adj_ddi, emb_dim\n",
    "            3. build a dynamic memory\n",
    "                - DynamicMemory\n",
    "                - use vocab_med\n",
    "            4. build a output network\n",
    "                - OutNet\n",
    "                - vocab_med, emb_dim\n",
    "        \"\"\"        \n",
    "        self.patient = PatientQuery(diag_vocab_size, emb_dim)\n",
    "        self.memorybank = MemoryBank(med_vocab_size, ehr_adj, emb_dim)\n",
    "        self.dynamicmemory = DynamicMemory(med_vocab_size)\n",
    "        self.outnet = OutNet(med_vocab_size, emb_dim)\n",
    "        \n",
    "    def forward(self, codes_diag, codes_med):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            codes_diag\n",
    "                - a list of length #visits\n",
    "                - each element is also itself a list\n",
    "            codes_prod\n",
    "                - a list of length #visits\n",
    "                - each element is also itself a list\n",
    "            codes_med\n",
    "                - a list of length #visits - 1\n",
    "                - each element is also itself a list\n",
    "        \"\"\"\n",
    "        \n",
    "        # get patient query embedding (#visit, emb_dim*2)\n",
    "        # get patient query embedding (#visit, emb_dim) ??\n",
    "        queries = self.patient(codes_diag)\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. build a memory bank, assign to MB\n",
    "                - use self.memorybank\n",
    "            2. build a dynamic memory, assign to DM_key and DM_value\n",
    "                - use self.dynamicmemory\n",
    "                - use queries and codes_med as features\n",
    "        \"\"\"\n",
    "        MB = None\n",
    "        DM_key, DM_value = None, None\n",
    "           \n",
    "        MB = self.memorybank()\n",
    "        DM_key, DM_value = self.dynamicmemory(queries, codes_med)\n",
    "        #print(\"queries\")\n",
    "        #print(queries.size())\n",
    "        # extract three memory outputs, assign to fact1, fact2, fact3\n",
    "        fact1 = Fact1(queries)()\n",
    "        #print(MB.size())\n",
    "        fact2 = Fact2(fact1, MB)()\n",
    "        fact3 = Fact3(fact1, MB, DM_key, DM_value)()\n",
    "\n",
    "        # get the final output\n",
    "        result = self.outnet(fact1, fact2, fact3)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAMENet(diag_vocab_size, med_vocab_size, ehr_adj)\n",
    "output = model([[0,1], [1,2], [4,5]], [[0,1], [2,5]])\n",
    "\n",
    "assert output.shape == (1, med_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[426, 97, 26, 51, 68, 256, 490, 1000], [954, 34, 68, 36, 26, 7, 71]], [[82, 83, 100, 101, 23, 37, 20, 5, 0, 45, 18, 117, 224], [34, 21, 0, 5, 18, 19, 20, 22, 4, 100, 101, 84, 41, 42, 9, 10, 11, 6, 2, 26, 27, 28, 29, 30, 81, 35, 149]]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "tmp = records.copy()\n",
    "random.shuffle(tmp)\n",
    "\n",
    "print(tmp[0])\n",
    "# 80%: 20% split training and test sets randomly\n",
    "train_set = tmp[:int(len(records)*0.8)]\n",
    "test_set = tmp[int(len(records)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426, 97, 26, 51, 68, 256, 490, 1000]\n",
      "patient list\n",
      "[[[426, 97, 26, 51, 68, 256, 490, 1000], [954, 34, 68, 36, 26, 7, 71]]]\n",
      "diag\n",
      "[426, 97, 26, 51, 68, 256, 490, 1000]\n",
      "med\n",
      "[954, 34, 68, 36, 26, 7, 71]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 954 is out of bounds for axis 1 with size 385",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jessi\\Documents\\Master's in Computer Science\\2022 - Semester 2 - Spring\\CS 598 Deep Learning in Healthcare\\Git\\NEW-G-BERT\\G-BERT\\Baselines\\GAMENet\\gamenet.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=81'>82</a>\u001b[0m         \u001b[39mprint\u001b[39m(accuracy_score(\u001b[39m*\u001b[39mtest(test_set[:\u001b[39m100\u001b[39m])))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=83'>84</a>\u001b[0m \u001b[39m# train the model with first 500 patients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=84'>85</a>\u001b[0m \u001b[39m# feel free to change the number of training patients\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=85'>86</a>\u001b[0m train(train_set[:\u001b[39m50\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\jessi\\Documents\\Master's in Computer Science\\2022 - Semester 2 - Spring\\CS 598 Deep Learning in Healthcare\\Git\\NEW-G-BERT\\G-BERT\\Baselines\\GAMENet\\gamenet.ipynb Cell 20'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_set)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(visit)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=49'>50</a>\u001b[0m \u001b[39m#print(\"meds\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=50'>51</a>\u001b[0m \u001b[39m#print(visit[0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=51'>52</a>\u001b[0m \u001b[39m#print(\"Diags\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=52'>53</a>\u001b[0m \u001b[39m#print(visit[1])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=53'>54</a>\u001b[0m \u001b[39m# get the training data and target of bce loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=54'>55</a>\u001b[0m codes_diag, codes_med, target \u001b[39m=\u001b[39m dataFormatter(patient[:idx\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=55'>56</a>\u001b[0m \u001b[39m# get the target of multilabel_margin_loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=56'>57</a>\u001b[0m multi_target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, med_vocab_size), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\jessi\\Documents\\Master's in Computer Science\\2022 - Semester 2 - Spring\\CS 598 Deep Learning in Healthcare\\Git\\NEW-G-BERT\\G-BERT\\Baselines\\GAMENet\\gamenet.ipynb Cell 20'\u001b[0m in \u001b[0;36mdataFormatter\u001b[1;34m(patient_list)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=13'>14</a>\u001b[0m     med_list\u001b[39m.\u001b[39mappend(med)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=15'>16</a>\u001b[0m target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, med_vocab_size))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=16'>17</a>\u001b[0m target[\u001b[39m0\u001b[39m, med_list[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/GAMENet/gamenet.ipynb#ch0000018?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m diag_list, med_list[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], torch\u001b[39m.\u001b[39mFloatTensor(target)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 954 is out of bounds for axis 1 with size 385"
     ]
    }
   ],
   "source": [
    "\n",
    "model = model = GAMENet(diag_vocab_size, med_vocab_size, ehr_adj)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def dataFormatter(patient_list):\n",
    "    print(\"patient list\")\n",
    "    print(patient_list)\n",
    "    diag_list, med_list = [], []\n",
    "    for diag, med in patient_list:\n",
    "        print(\"diag\")\n",
    "        print(diag)\n",
    "        print(\"med\")\n",
    "        print(med)\n",
    "        diag_list.append(diag)\n",
    "        med_list.append(med)\n",
    "    \n",
    "    target = np.zeros((1, med_vocab_size))\n",
    "    target[0, med_list[-1]] = 1\n",
    "    return diag_list, med_list[:-1], torch.FloatTensor(target)\n",
    "\n",
    "# def dataFormatter(patient_list):\n",
    "#     diag_list, med_list = [], []\n",
    "\n",
    "#     for codes in patient_list:\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(test_set):\n",
    "    model.eval()\n",
    "    pred_list, target_list = [], []\n",
    "    for patient in test_set:\n",
    "        for idx, visit in enumerate(patient):\n",
    "            codes_diag, codes_med, target = dataFormatter(patient[:idx+1])\n",
    "            pred = model(codes_diag, codes_med).detach().cpu().numpy()[0]\n",
    "            pred[pred >= 0.5] = 1; pred[pred < 0.5] = 0\n",
    "            pred_list += pred.tolist(); target_list += target.numpy().tolist()[0]\n",
    "    return pred_list, target_list\n",
    "\n",
    "def train(train_set):\n",
    "    for i in range(2):\n",
    "        model.train()\n",
    "        for patient in train_set:\n",
    "            loss = 0\n",
    "\n",
    "            # codes_diag, codes_med, target = dataFormatter(patient)\n",
    "            # print(patient)\n",
    "            # Gets number of visits from diagnoses codes (should also be the same for medication codes)\n",
    "            for idx, visit in enumerate(patient[0]):\n",
    "                print(visit)\n",
    "                #print(\"meds\")\n",
    "                #print(visit[0])\n",
    "                #print(\"Diags\")\n",
    "                #print(visit[1])\n",
    "                # get the training data and target of bce loss\n",
    "                codes_diag, codes_med, target = dataFormatter(patient[:idx+1])\n",
    "                # get the target of multilabel_margin_loss\n",
    "                multi_target = np.full((1, med_vocab_size), -1)\n",
    "                for idx, item in enumerate(visit[2]):\n",
    "                    multi_target[0][idx] = item\n",
    "                multi_target = torch.LongTensor(multi_target)\n",
    "                \n",
    "                \"\"\"\n",
    "                TODO: get the output of the model, assign to pred\n",
    "                \"\"\"\n",
    "                pred = None\n",
    "                pred = model(codes_diag, codes_med)\n",
    "                \n",
    "                loss += F.binary_cross_entropy_with_logits(pred, target) + \\\n",
    "                                    F.multilabel_margin_loss(torch.sigmoid(pred), multi_target)\n",
    "            \n",
    "            \"\"\" \n",
    "            TODO:\n",
    "                1. set zero grad to the optimizer\n",
    "                2. backward loss\n",
    "                3. make one step of the optimizer\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # use the first 100 test patients to compute intermediate accuracy\n",
    "        print(accuracy_score(*test(test_set[:100])))\n",
    "    \n",
    "# train the model with first 500 patients\n",
    "# feel free to change the number of training patients\n",
    "train(train_set[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "# import pickle as pkl\n",
    "\n",
    "# gamenet_data_path = '../GAMENet_Data'\n",
    "# voc_file = 'voc_final.pkl'\n",
    "# file_obj = open(os.path.join(gamenet_data_path, voc_file), 'rb')\n",
    "# med_voc = pkl.load(file_obj)\n",
    "# med_voc_size = len(med_voc.idx2word)\n",
    "\n",
    "# ehr_adj = np.zeros((med_voc_size, med_voc_size))\n",
    "# for patient in records:\n",
    "#     for adm in patient:\n",
    "#         med_set = adm[2]\n",
    "#         for i, med_i in enumerate(med_set):\n",
    "#             for j, med_j in enumerate(med_set):\n",
    "#                 if j<=i:\n",
    "#                     continue\n",
    "#                 ehr_adj[med_i, med_j] = 1\n",
    "#                 ehr_adj[med_j, med_i] = 1\n",
    "# print(ehr_adj)\n",
    "# #dill.dump(ehr_adj, open('ehr_adj_final.pkl', 'wb'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "# records = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_PKL))\n",
    "\n",
    "# # Retrieve unique medication codes and diagnosis codes\n",
    "# med_vocab = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ATC_CSV))[ATC4]\n",
    "# diag_vocab = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ICD_CSV))[ICD9_CODE]\n",
    "\n",
    "# med_vocab = med_vocab.values.tolist()\n",
    "# diag_vocab = diag_vocab.values.tolist()\n",
    "# #print(med_vocab.values.tolist())\n",
    "\n",
    "# print(records.head(5))\n",
    "# print(records['ATC4'].head(5))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
