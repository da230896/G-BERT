{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from Globals import MULTI_VISIT_PKL, UNIQUE_ATC_CSV, UNIQUE_ICD_CSV\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.nn import Sequential, GCNConv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_DATA_PATH = \"../Data\"\n",
    "ATC4 = \"ATC4\"\n",
    "ICD9_CODE = \"ICD9_CODE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUBJECT_ID  HADM_ID                                               ATC4  \\\n",
      "0           4   185777  [A12BA, B05XA, N02BE, A10AE, R05DB, R05DA, R05...   \n",
      "1           6   107064  [A12AA, D11AX, C07AB, C08CA, D11AH, L04AD, C03...   \n",
      "2           9   150750  [A12BA, B05XA, J01MA, S01AE, C07AB, C02DB, A01...   \n",
      "3          11   194540  [B01AB, C05BA, S01XA, A06AA, A06AB, N02BE, N02...   \n",
      "4          12   112213  [N05BA, A12AA, D11AX, B01AB, C05BA, S01XA, C07...   \n",
      "\n",
      "                                           ICD9_CODE  \n",
      "0  [042, 1363, 7994, 2763, 7907, 5715, 04111, V09...  \n",
      "1  [40391, 4440, 9972, 2766, 2767, 2859, 2753, V1...  \n",
      "2                [431, 5070, 4280, 5849, 2765, 4019]  \n",
      "3                                             [1913]  \n",
      "4  [1570, 57410, 9971, 4275, 99811, 4019, 5680, 5...  \n",
      "0    [A12BA, B05XA, N02BE, A10AE, R05DB, R05DA, R05...\n",
      "1    [A12AA, D11AX, C07AB, C08CA, D11AH, L04AD, C03...\n",
      "2    [A12BA, B05XA, J01MA, S01AE, C07AB, C02DB, A01...\n",
      "3    [B01AB, C05BA, S01XA, A06AA, A06AB, N02BE, N02...\n",
      "4    [N05BA, A12AA, D11AX, B01AB, C05BA, S01XA, C07...\n",
      "Name: ATC4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "records = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_PKL))\n",
    "\n",
    "# Retrieve unique medication codes and diagnosis codes\n",
    "med_vocab = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ATC_CSV))[ATC4]\n",
    "diag_vocab = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ICD_CSV))[ICD9_CODE]\n",
    "\n",
    "med_vocab = med_vocab.values.tolist()\n",
    "diag_vocab = diag_vocab.values.tolist()\n",
    "#print(med_vocab.values.tolist())\n",
    "\n",
    "print(records.head(5))\n",
    "print(records['ATC4'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 664: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jessi\\Documents\\Master's in Computer Science\\2022 - Semester 2 - Spring\\CS 598 Deep Learning in Healthcare\\Git\\NEW-G-BERT\\G-BERT\\Baselines\\gamenet.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000003?line=4'>5</a>\u001b[0m voc_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvoc_final.pkl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000003?line=5'>6</a>\u001b[0m file_obj \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(gamenet_data_path, voc_file), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000003?line=6'>7</a>\u001b[0m med_voc \u001b[39m=\u001b[39m pkl\u001b[39m.\u001b[39;49mload(file_obj)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000003?line=7'>8</a>\u001b[0m med_voc_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(med_voc\u001b[39m.\u001b[39midx2word)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000003?line=9'>10</a>\u001b[0m ehr_adj \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((med_voc_size, med_voc_size))\n",
      "File \u001b[1;32mC:\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python310/lib/encodings/cp1252.py?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> <a href='file:///c%3A/Python310/lib/encodings/cp1252.py?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 664: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pickle as pkl\n",
    "\n",
    "gamenet_data_path = '../GAMENet_Data'\n",
    "voc_file = 'voc_final.pkl'\n",
    "file_obj = open(os.path.join(gamenet_data_path, voc_file), 'rb')\n",
    "med_voc = pkl.load(file_obj)\n",
    "med_voc_size = len(med_voc.idx2word)\n",
    "\n",
    "ehr_adj = np.zeros((med_voc_size, med_voc_size))\n",
    "for patient in records:\n",
    "    for adm in patient:\n",
    "        med_set = adm[2]\n",
    "        for i, med_i in enumerate(med_set):\n",
    "            for j, med_j in enumerate(med_set):\n",
    "                if j<=i:\n",
    "                    continue\n",
    "                ehr_adj[med_i, med_j] = 1\n",
    "                ehr_adj[med_j, med_i] = 1\n",
    "print(ehr_adj)\n",
    "#dill.dump(ehr_adj, open('ehr_adj_final.pkl', 'wb'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jessi\\Documents\\Master's in Computer Science\\2022 - Semester 2 - Spring\\CS 598 Deep Learning in Healthcare\\Git\\NEW-G-BERT\\G-BERT\\Baselines\\gamenet.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000013?line=8'>9</a>\u001b[0m             \u001b[39mfor\u001b[39;00m j, code_j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(visit):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000013?line=9'>10</a>\u001b[0m                 \u001b[39mif\u001b[39;00m j \u001b[39m>\u001b[39m i:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000013?line=10'>11</a>\u001b[0m                     adj_matrix[code_i, code_j] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000013?line=11'>12</a>\u001b[0m                     adj_matrix[code_j, code_i] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jessi/Documents/Master%27s%20in%20Computer%20Science/2022%20-%20Semester%202%20-%20Spring/CS%20598%20Deep%20Learning%20in%20Healthcare/Git/NEW-G-BERT/G-BERT/Baselines/gamenet.ipynb#ch0000013?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(adj_matrix)\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# Create adjacency matrix from medication codes\n",
    "N = len(med_vocab)\n",
    "\n",
    "adj_matrix = torch.zeros(N, N)\n",
    "\n",
    "for patient in records[ATC4]:\n",
    "    for visit in patient:\n",
    "        for i, code_i in enumerate(visit):\n",
    "            for j, code_j in enumerate(visit):\n",
    "                if j > i:\n",
    "                    adj_matrix[code_i, code_j] = 1\n",
    "                    adj_matrix[code_j, code_i] = 1\n",
    "\n",
    "print(adj_matrix)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim)\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, codes):        \n",
    "        emb_list = []\n",
    "        for code in codes:\n",
    "            emb = None\n",
    "            emb = self.embeddings(torch.tensor(code))\n",
    "            \n",
    "            # take the mean and make one sample per batch\n",
    "            emb_mean = emb.mean(dim=0).unsqueeze(dim=0)\n",
    "            emb_list.append(emb_mean)\n",
    "            \n",
    "        emb_seq = torch.cat(emb_list, dim=0).unsqueeze(dim=0)\n",
    "        \n",
    "        result, _ = self.rnn(emb_seq)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.normal_(self.embeddings.weight)\n",
    "        for param in self.rnn.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            else:\n",
    "                torch.nn.init.normal_(param.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientQuery(nn.Module):\n",
    "    def __init__(self, diag_vocab, emb_dim=16):\n",
    "        super(PatientQuery, self).__init__()\n",
    "        self.diag_rnn = RNN(diag_vocab, emb_dim)\n",
    "        self.linear = nn.Linear(in_features=emb_dim * 2, out_features=emb_dim, bias=True)\n",
    "        \n",
    "    def forward(self, codes_diag,):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            codes_diag: [[diag codes for visit1], [diag codes for visit2], ...]\n",
    "                - e.g., [[0,1], [1,2]]  \n",
    "            codes_prod: [[prod codes for visit1], [prod codes for visit2], ...]\n",
    "                - e.g., [[0,1], [1,2]]  \n",
    "        output:\n",
    "            query embedding:\n",
    "                - size: (#visits, emd_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: get diag and prod embedding by self.rnn_diag, self.rnn_prod\n",
    "        \"\"\"\n",
    "        diag_emb = self.diag_rnn(codes_diag)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: concat emb_diag and emb_prod, then tranfrom by self.linear\n",
    "        \"\"\"\n",
    "        emb_cat = None\n",
    "        result = None\n",
    "        \n",
    "        #emb_cat = torch.cat((emb_diag, emb_prod), 2)\n",
    "        #result = self.linear(emb_cat)\n",
    "        #result = result.squeeze(0)\n",
    "        #print(result.size())\n",
    "        #return result\n",
    "\n",
    "        return diag_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, vocab_med, adj, emb_dim=16):\n",
    "        super(GCN, self).__init__()\n",
    "        adj = self.normalize(adj + np.eye(adj.shape[0]))\n",
    "        self.adj = torch.FloatTensor(adj)\n",
    "        \n",
    "        self.gcn1 = GCNConv(in_channels=vocab_med, out_channels=emb_dim)\n",
    "        self.gcn2 = GCNConv(in_channels=emb_dim, out_channels=emb_dim)\n",
    "        \n",
    "        # self.gcn1 = GraphConvolution(in_features=vocab_med, out_features=emb_dim)\n",
    "        # self.gcn2 = GraphConvolution(in_features=emb_dim, out_features=emb_dim)\n",
    "        \n",
    "        # the initial feature\n",
    "        self.x = torch.eye(vocab_med)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1 (already done). use self.gcn1 for the first graph convolution\n",
    "                - remember to use self.adj\n",
    "            2. use F.relu() as the activation function\n",
    "            3. use self.gcn2\n",
    "        \"\"\"\n",
    "        x = self.gcn1(self.x, self.adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, self.adj)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def normalize(self, adj):\n",
    "        adj = adj / (adj @ np.ones(adj.shape) + 1e-8)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank(nn.Module):\n",
    "    def __init__(self, vocab_med, adj_ehr, adj_ddi, emb_dim=16):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: conbime information from EHR graph and DDI graph\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. use GCN for EHR graph\n",
    "                - using adj_ehr\n",
    "                - using emb_dim as the ouput dimension\n",
    "            2. use GCN for DDI graph\n",
    "                - using adj_ddi\n",
    "                - using emb_dim as the ouput dimension\n",
    "            3 (already done). design a learnable weight between them\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gcn_ehr = GCN(vocab_med=vocab_med, adj=adj_ehr, emb_dim=emb_dim)\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. get ehr graph feature\n",
    "                - using .forward() function\n",
    "            2. get ddi graph feature\n",
    "                - using .forward() function\n",
    "            3 (already done). get weighted information\n",
    "        \"\"\"\n",
    "        \n",
    "        info_ehr = self.gcn_ehr.forward()\n",
    "        \n",
    "        info_comb = info_ehr * self.weight\n",
    "        \n",
    "        return info_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicMemory(nn.Module):\n",
    "    def __init__(self, vocab_med):\n",
    "        super(DynamicMemory, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: generate a historical mapping: query embedding -> multi-hot medication vector\n",
    "        \"\"\"\n",
    "        self.vocab_med = vocab_med\n",
    "    \n",
    "    def forward(self, queries, codes_med):\n",
    "        \"\"\"\n",
    "        Input:  queries\n",
    "                    - this is the historical query embedding, given by PatientQuery Module\n",
    "                    - size: (#visits - 1, emb_dim), delete the current query\n",
    "                codes_med\n",
    "                    - this is the historical groud truth med vector\n",
    "                    - format: a list of length (#visits - 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        DM_key = queries\n",
    "        DM_value = np.zeros((queries.shape[0], self.vocab_med))\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: transform codes_med to multi-hot and filling DM_value row by row (visit by visit)\n",
    "        \"\"\"\n",
    "        #print(DM_key)\n",
    "        #print(codes_med)\n",
    "        #print(DM_value)\n",
    "        for i in range(len(codes_med)):\n",
    "            for j in codes_med[i]:\n",
    "                DM_value[i][j] = 1\n",
    "        \n",
    "        # use torch.FloatTensor for DM_value\n",
    "        DM_value = torch.FloatTensor(DM_value)\n",
    "        \n",
    "        return DM_key, DM_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fact1(nn.Module):\n",
    "    def __init__(self, queries):\n",
    "        super(Fact1, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: extract the final embedding from input queries, q^t\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: assign the last embedding to result\n",
    "                - final size: (1, emb_dim)\n",
    "                - make sure the output size is **not** a vector format (emb_dim,)\n",
    "        \"\"\"\n",
    "        result = None\n",
    "        #print(\"from fact1, queries size\")\n",
    "        #print(self.queries.size())\n",
    "        result = self.queries[-1]\n",
    "        \n",
    "        #print(\"from fact1, result size\")\n",
    "        #print(result.size())\n",
    "        result = result.unsqueeze(0)\n",
    "        #print(result.size())\n",
    "        return result\n",
    "    \n",
    "\n",
    "class Fact2(nn.Module):\n",
    "    def __init__(self, query, MB):\n",
    "        super(Fact2, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: get attention information from MB, o^t_b\n",
    "        Input:\n",
    "            query\n",
    "                - this is the last embedding\n",
    "            MB\n",
    "                - is the memory bank\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. get the attention weight between query and each row of MB, adding to attn_score\n",
    "                - use torch.mm()\n",
    "                - tip: need to transpose MB, using MB.t()\n",
    "            2. use F.softmax(, dim=1) to compute the attention matrix, attn_matrix\n",
    "            3. get the final result from attn_matrix and MB\n",
    "                - use torch.mm(,)\n",
    "        \"\"\"\n",
    "        attn_score = None\n",
    "        attn_matrix = None\n",
    "        result = None\n",
    "        #print(\"query\")\n",
    "        #print(self.query.size())\n",
    "        #print(\"mb\")\n",
    "        #print(self.MB.size())\n",
    "        attn_score = torch.mm(self.query, self.MB.t())\n",
    "        attn_matrix = F.softmax(attn_score, dim=1)\n",
    "        result = torch.mm(attn_matrix, self.MB)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "class Fact3(nn.Module):\n",
    "    def __init__(self, query, MB, DM_key, DM_value):\n",
    "        super(Fact3, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: similar to Fact2, get information from the DM, o_d^t\n",
    "        \"\"\"\n",
    "        self.query = query\n",
    "        self.MB = MB\n",
    "        self.DM_key = DM_key\n",
    "        self.DM_value = DM_value\n",
    "    \n",
    "    def forward(self):\n",
    "        attn = F.softmax(torch.mm(self.query, self.DM_key.t()), dim=1)\n",
    "        value = torch.mm(attn, self.DM_value)\n",
    "        out = torch.mm(value, self.MB)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutNet(nn.Module):\n",
    "    def __init__(self, vocab_med, emb_dim=16):\n",
    "        super(OutNet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: combine fact1, fact2, fact3 to do final prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. build the first linear layer\n",
    "                - input size: 3 * emb_dim\n",
    "                - end size: 2 * emb_dim\n",
    "                - use bias=True option\n",
    "            2. build the second linear layer\n",
    "                - input size: 2 * emb_dim\n",
    "                - end size: vocab_med\n",
    "                - use bias=True option\n",
    "        \"\"\"\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=3 * emb_dim, out_features=2 * emb_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=2 * emb_dim, out_features=vocab_med, bias=True)\n",
    "        \n",
    "    def forward(self, fact1, fact2, fact3):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            fact1, fact2, fact3:\n",
    "                - three facts q^t, o^t_b, o^t_d\n",
    "                - each size: (1, emb_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. concat fact1, fact2, fact3, and assign to memory_out\n",
    "                - size: (1, 3 * emb_dim)\n",
    "            2. pass through the first linear layer\n",
    "            3. use F.relu() as the activation\n",
    "            3. pass through the second linear layer\n",
    "        \"\"\"\n",
    "        memory_out = None\n",
    "        result = None\n",
    "        \n",
    "        memory_out = torch.cat((fact1, fact2, fact3), dim=1)\n",
    "        #print(memory_out.size())\n",
    "        \n",
    "        result = self.fc1(memory_out)\n",
    "        result = F.relu(result)\n",
    "        result = self.fc2(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAMENet(nn.Module):\n",
    "    def __init__(self, vocab_diag, vocab_prod, vocab_med, adj_ehr, adj_ddi, emb_dim=128):\n",
    "        super(GAMENet, self).__init__()\n",
    "        \"\"\"\n",
    "        FUNCTIONALITY: integrate the whole model together\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. build a patient query network\n",
    "                - PatientQuery\n",
    "                - use vocab_diag, vocab_prod, emb_dim\n",
    "            2. build a memory bank\n",
    "                - MemoryBank\n",
    "                - use vocab_med, adj_ehr, adj_ddi, emb_dim\n",
    "            3. build a dynamic memory\n",
    "                - DynamicMemory\n",
    "                - use vocab_med\n",
    "            4. build a output network\n",
    "                - OutNet\n",
    "                - vocab_med, emb_dim\n",
    "        \"\"\"\n",
    "        self.patient = None\n",
    "        self.memorybank = None\n",
    "        self.dynamicmemory = None\n",
    "        self.outnet = None\n",
    "        \n",
    "        self.patient = PatientQuery(vocab_diag, vocab_prod, emb_dim)\n",
    "        self.memorybank = MemoryBank(vocab_med, adj_ehr, adj_ddi, emb_dim)\n",
    "        self.dynamicmemory = DynamicMemory(vocab_med)\n",
    "        self.outnet = OutNet(vocab_med, emb_dim)\n",
    "        \n",
    "    def forward(self, codes_diag, codes_prod, codes_med):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            codes_diag\n",
    "                - a list of length #visits\n",
    "                - each element is also itself a list\n",
    "            codes_prod\n",
    "                - a list of length #visits\n",
    "                - each element is also itself a list\n",
    "            codes_med\n",
    "                - a list of length #visits - 1\n",
    "                - each element is also itself a list\n",
    "        \"\"\"\n",
    "        \n",
    "        # get patient query embedding (#visit, emb_dim*2)\n",
    "        queries = self.patient(codes_diag, codes_prod)\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. build a memory bank, assign to MB\n",
    "                - use self.memorybank\n",
    "            2. build a dynamic memory, assign to DM_key and DM_value\n",
    "                - use self.dynamicmemory\n",
    "                - use queries and codes_med as features\n",
    "        \"\"\"\n",
    "        MB = None\n",
    "        DM_key, DM_value = None, None\n",
    "           \n",
    "        MB = self.memorybank()\n",
    "        DM_key, DM_value = self.dynamicmemory(queries, codes_med)\n",
    "        #print(\"queries\")\n",
    "        #print(queries.size())\n",
    "        # extract three memory outputs, assign to fact1, fact2, fact3\n",
    "        fact1 = Fact1(queries)()\n",
    "        #print(MB.size())\n",
    "        fact2 = Fact2(fact1, MB)()\n",
    "        fact3 = Fact3(fact1, MB, DM_key, DM_value)()\n",
    "\n",
    "        # get the final output\n",
    "        result = self.outnet(fact1, fact2, fact3)\n",
    "        \n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
