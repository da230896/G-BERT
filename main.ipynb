{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from typing import Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tracemalloc\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn.conv import GATConv\n",
    "from torch_geometric.nn.inits import glorot\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import time\n",
    "\n",
    "from Utils import multi_label_metric  \n",
    "\n",
    "random.seed(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline\">G-BERT</span>\n",
    "\n",
    "Note that ATC vocab and ICD vocab are generated in PART-2: (_get_atc_ontology and _get_icd_ontology) and to be used overall[Can be revisited if needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have vocab class, we can define a utility method to get multi-hot vector using vocab and incoming list of codes. This is pure multi-hot-vector in the sense that it do not contain special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_hot(code_list: Union[list[str], list[list[str]]], vocab: Vocab)->list[int]:\n",
    "    if len(code_list) == 0:\n",
    "        raise Exception(\"Should have codes to convert\")\n",
    "    if isinstance(code_list[0], list):\n",
    "        zeros_and_ones = torch.zeros((len(code_list), len(vocab.word2idx)), dtype=torch.float)\n",
    "        for i, codes in enumerate(code_list):\n",
    "            for code in codes: \n",
    "                if vocab.word2idx.get(code) == None:\n",
    "                    pass\n",
    "                    # TODO: Need to check these pass \n",
    "                    # print(\"[get_multi_hot]: has some unseen code\", code)\n",
    "                else:\n",
    "                    zeros_and_ones[i, vocab.word2idx.get(code)] = 1\n",
    "    else:    \n",
    "        zeros_and_ones = torch.zeros(len(vocab.word2idx), dtype=torch.float)\n",
    "        for code in code_list:\n",
    "            if vocab.word2idx.get(code) == None:\n",
    "                pass\n",
    "                # TODO: Need to check these pass\n",
    "                # print(\"[get_multi_hot]: has some unseen code\", code)\n",
    "            else:\n",
    "                zeros_and_ones[vocab.word2idx.get(code)] = 1\n",
    "            \n",
    "    return zeros_and_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Global Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\") \n",
    "    torch.cuda.device(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-1: Filtering MIMIC-III for Unique ATC/ICD</u>\n",
    "\n",
    "<a href=\"https://mimic.mit.edu/docs/iii/\">MIMIC-III</a> has PRESCRIPTIONS.csv which has NDC medication codes. So we would need mapping to ATC codes.\n",
    "\n",
    "Mapping is done using RX-Norm API. First NDC is converted to RXCUI(getNDCStatus) and then to ATC code(rx-norm class API). This all is done in an R script here: https://github.com/fabkury/ndc_map\n",
    "\n",
    "Steps for _atc_ in an ideal situation would be:\n",
    "<ol>\n",
    "    <li>Parse output of the R-Script mentioned above to create an NDC to ATC map</li>\n",
    "    <li>Use that map to find out ATC of PRESCRIPTION.csv [<i><a href=\"https://github.com/jshang123/G-Bert\">G-BERT author's implementation</a></i> has first visit only][can this qualify as an extension?]</li>\n",
    "    <li>Store unique atc codes in atc4-vocab.csv</li>\n",
    "</ol>\n",
    "\n",
    "But since the Prescriptions.csv is pretty old and many NDC codes have changed we are using <i>ndc2rxnorm_mapping,csv</i> and <i>rxnorm2atc_level4.csv</i> which are obtained from <a href=\"https://github.com/sjy1203/GAMENet\">GAMENET implementation</a>. Thus the new steps:\n",
    "\n",
    "<ol>\n",
    "<li>Load NDC codes from Prescriptions.csv(MIMIC-III)</li>\n",
    "<li>Convert them to RXNorm codes using corresponding mapping file</li>\n",
    "<li>Convert RXNorm codes to ATC codes</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_prescriptions_and_mapping_files() -> pd.DataFrame:\n",
    "    # TODO: need to reduce the unique codes and validate the mapping files\n",
    "    # step 1\n",
    "    print(\"[merge_prescriptions_and_mapping_files] Begining of loading MIMIC-III PRESCRIPTIONS.csv\")\n",
    "    prescriptions_df = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, PRESCRIPTIONS))\n",
    "    medications = prescriptions_df.loc[~(prescriptions_df[\"NDC\"]==0.0)].astype({\"NDC\": \"Int64\"})\n",
    "    medications.dropna(inplace=True)\n",
    "\n",
    "    # step 2\n",
    "    print(\"[merge_prescriptions_and_mapping_files] Begining of loading NDC-RXCUI mapping for merging prescriptions\")\n",
    "    ndc2rx_cui:dict[str, str] = {}\n",
    "    with open(os.path.join(GLOBAL_DATA_PATH, NDC_2_RXCUI_MAPPING), 'r') as f:\n",
    "        for key, value in eval(f.read()).items():\n",
    "            try:\n",
    "                ndc2rx_cui[int(key)] = value \n",
    "            except Exception:\n",
    "                print(\"[merge_prescriptions_and_mapping_files]\", key, value, \" is causing some issue while loading NDC-RXCUI mapping\")\n",
    "    medications[\"RXCUI\"] = medications[\"NDC\"].map(ndc2rx_cui, na_action=\"ignore\")\n",
    "    medications.drop(index = (medications.loc[medications[\"RXCUI\"] == \"\"]).index, axis=0, inplace=True)\n",
    "    medications[\"RXCUI\"] = medications[\"RXCUI\"].astype(\"int64\")\n",
    "\n",
    "    # step 3:\n",
    "    print(\"[merge_prescriptions_and_mapping_files] Begining of loading RXCUI-ATC mapping for merging prescriptions\")\n",
    "    rxcui2atc = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, RXCUI_2_ATC))\n",
    "    return pd.merge(medications[[\"SUBJECT_ID\", \"HADM_ID\", \"RXCUI\"]], rxcui2atc[[\"RXCUI\", \"ATC4\"]], on=[\"RXCUI\"])\n",
    "\n",
    "def create_unique_atc_csv_file(outputPath: str) -> pd.DataFrame:\n",
    "    print(\"[create_unique_atc_csv_file] Starting...\")\n",
    "    prescriptions = merge_prescriptions_and_mapping_files()\n",
    "    print(\"[create_unique_atc_csv_file] Unique ATC codes in hand now\")\n",
    "    atc4_codes = prescriptions[\"ATC4\"].drop_duplicates().reset_index()\n",
    "    print(\"[create_unique_atc_csv_file] Going to store ATC codes now\")\n",
    "    atc4_codes.to_csv(outputPath)\n",
    "\n",
    "    return atc4_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create atc vocab <br>\n",
    "<span style=\"color: #055BA6\">create_unique_atc_csv_file(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ATC_CSV))</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for _DX_ would be:\n",
    "<ol>\n",
    "    <li>Use DIAGNOSES_ICD.csv column \"ICD9_CODE\"</li>\n",
    "    <li>Step 2-3: Store unique ICD codes in unique-icd.csv. <a href=\"https://arxiv.org/pdf/1906.00346.pdf\">G-BERT</a> implementation on <a href=\"https://github.com/jshang123/G-Bert\">github</a> process and stores only top 2000 diagnosis(by freq). <span style=\"color: #FF540D\">[Assuming] Since not in paper</span></li>\n",
    "</ol>\n",
    "Moreover, we analyzed that only 3% visits have >30% \"[UNK]\" tokens if we take top 2000. This means that 97% of data has > 70% diagnosis per visit within top 2000 diagnosis.\n",
    "\n",
    "<span style=\"color: #FF540D\">[Issue] Author implementation has two vocabs but ideally BERT style pre-training should have one global vocab</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_icd_csv_file(outputPath: str) -> pd.DataFrame:\n",
    "    # step 1\n",
    "    print(\"[create_unique_icd_csv_file] Step 1... start\")\n",
    "    diagnosis_df = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, DIAGNOSIS_ICD))\n",
    "    diagnosis_df.dropna(inplace=True)\n",
    "    diagnosis_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Step 2: to find top 2000 by freq of occurrence\n",
    "    print(\"[create_unique_icd_csv_file] Step 2... start\")\n",
    "    icd_codes = diagnosis_df[\"ICD9_CODE\"]\n",
    "    icd_codes = icd_codes.groupby(icd_codes).count().to_frame(name=\"ICD9_CODE_COUNT\").reset_index()\n",
    "    sorted_icd_codes = icd_codes.sort_values(\"ICD9_CODE_COUNT\", ascending=False).iloc[:2000]\n",
    "\n",
    "    sorted_icd_codes.drop_duplicates(subset=[\"ICD9_CODE\"], inplace=True)\n",
    "\n",
    "    #step 3\n",
    "    print(\"[create_unique_icd_csv_file] Step 3... start\")\n",
    "    sorted_icd_codes[\"ICD9_CODE\"].reset_index(drop=True).astype(\"string\").to_csv(outputPath)\n",
    "\n",
    "    return sorted_icd_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create ICD vocab <br>\n",
    "<span style=\"color: #055BA6\">create_unique_icd_csv_file(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ICD_CSV))</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-2: Defining Functions that can create Ontologies/trees(medication(ATC) and diseases(ICD9))</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have list of ICD codes available(hierarchical) here: https://icd.codes/icd9cm\n",
    "\n",
    "Based on this list and knowledge, below is a method which <i>identify</i> what would be ancestor given an ICD code. After that we can simply create a tree[Refer the second method in the code below] of icd codes which we will call an Ontology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ICD-9\n",
    "\"\"\"\n",
    "\n",
    "def level2_ancestors() ->  dict[str, str]:\n",
    "    level2 = [\"001-009\", \"010-018\", \"020-027\", \"030-041\", \"042\", \"045-049\", \"050-059\", \"060-066\", \"070-079\", \"080-088\",\n",
    "              \"090-099\", \"100-104\", \"110-118\", \"120-129\", \"130-136\", \"137-139\", \"140-149\", \"150-159\", \"160-165\",\n",
    "              \"170-176\",\n",
    "              \"176\", \"179-189\", \"190-199\", \"200-208\", \"209\", \"210-229\", \"230-234\", \"235-238\", \"239\", \"240-246\",\n",
    "              \"249-259\",\n",
    "              \"260-269\", \"270-279\", \"280-289\", \"290-294\", \"295-299\", \"300-316\", \"317-319\", \"320-327\", \"330-337\", \"338\",\n",
    "              \"339\", \"340-349\", \"350-359\", \"360-379\", \"380-389\", \"390-392\", \"393-398\", \"401-405\", \"410-414\", \"415-417\",\n",
    "              \"420-429\", \"430-438\", \"440-449\", \"451-459\", \"460-466\", \"470-478\", \"480-488\", \"490-496\", \"500-508\",\n",
    "              \"510-519\",\n",
    "              \"520-529\", \"530-539\", \"540-543\", \"550-553\", \"555-558\", \"560-569\", \"570-579\", \"580-589\", \"590-599\",\n",
    "              \"600-608\",\n",
    "              \"610-611\", \"614-616\", \"617-629\", \"630-639\", \"640-649\", \"650-659\", \"660-669\", \"670-677\", \"678-679\",\n",
    "              \"680-686\",\n",
    "              \"690-698\", \"700-709\", \"710-719\", \"720-724\", \"725-729\", \"730-739\", \"740-759\", \"760-763\", \"764-779\",\n",
    "              \"780-789\",\n",
    "              \"790-796\", \"797-799\", \"800-804\", \"805-809\", \"810-819\", \"820-829\", \"830-839\", \"840-848\", \"850-854\",\n",
    "              \"860-869\",\n",
    "              \"870-879\", \"880-887\", \"890-897\", \"900-904\", \"905-909\", \"910-919\", \"920-924\", \"925-929\", \"930-939\",\n",
    "              \"940-949\",\n",
    "              \"950-957\", \"958-959\", \"960-979\", \"980-989\", \"990-995\", \"996-999\", \"V01-V91\", \"V01-V09\", \"V10-V19\",\n",
    "              \"V20-V29\",\n",
    "              \"V30-V39\", \"V40-V49\", \"V50-V59\", \"V60-V69\", \"V70-V82\", \"V83-V84\", \"V85\", \"V86\", \"V87\", \"V88\", \"V89\",\n",
    "              \"V90\",\n",
    "              \"V91\", \"E000-E899\", \"E000\", \"E001-E030\", \"E800-E807\", \"E810-E819\", \"E820-E825\", \"E826-E829\", \"E830-E838\",\n",
    "              \"E840-E845\", \"E846-E849\", \"E850-E858\", \"E860-E869\", \"E870-E876\", \"E878-E879\", \"E880-E888\", \"E890-E899\",\n",
    "              \"E900-E909\", \"E910-E915\", \"E916-E928\", \"E929\", \"E930-E949\", \"E950-E959\", \"E960-E969\", \"E970-E978\",\n",
    "              \"E980-E989\", \"E990-E999\"]\n",
    "\n",
    "    level2_ancestor = {}\n",
    "    for i in level2:\n",
    "        tokens = i.split(\"-\")\n",
    "        if i[0] == \"V\":\n",
    "            if len(tokens) == 1:\n",
    "                level2_ancestor[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0][1:]), int(tokens[1][1:]) + 1):\n",
    "                    level2_ancestor[\"V%02d\" % j] = i\n",
    "        elif i[0] == \"E\":\n",
    "            if len(tokens) == 1:\n",
    "                level2_ancestor[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0][1:]), int(tokens[1][1:]) + 1):\n",
    "                    level2_ancestor[\"E%03d\" % j] = i\n",
    "        else:\n",
    "            if len(tokens) == 1:\n",
    "                level2_ancestor[i] = i\n",
    "            else:\n",
    "                for j in range(int(tokens[0]), int(tokens[1]) + 1):\n",
    "                    level2_ancestor[\"%03d\" % j] = i\n",
    "    return level2_ancestor\n",
    "\n",
    "\n",
    "def build_icd9_tree(unique_codes:list[str]) -> Tuple[list[str], Vocab]:\n",
    "    paths = []\n",
    "    icd9_vocab = Vocab()\n",
    "    icd9_leaf_vocab = Vocab()\n",
    "\n",
    "    root_node = \"icd9_root\"\n",
    "    level3_dict = level2_ancestors()\n",
    "    for code in unique_codes:\n",
    "        level1 = code\n",
    "        icd9_leaf_vocab.add_sentence([code])\n",
    "        level2 = level1[:4] if level1[0] == \"E\" else level1[:3]\n",
    "        level3 = level3_dict[level2]\n",
    "        level4 = root_node\n",
    "\n",
    "        path = [level1, level2, level3, level4]\n",
    "\n",
    "        icd9_vocab.add_sentence(path)\n",
    "        paths.append(path)\n",
    "\n",
    "    return paths, icd9_vocab, icd9_leaf_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for ATC codes. Now to understand ATC hierarchy these are the resources:\n",
    "<ul>\n",
    "    <li><a href=\"https://www.whocc.no/atc/structure_and_principles/\">https://www.whocc.no/atc/structure_and_principles/<a></li>\n",
    "    <li><a href=\"https://www.atccode.com/\">https://www.atccode.com/</a></li>\n",
    "</ul>\n",
    "\n",
    "We are using ATC since its mentioned in the paper. Moreover several papers consider ATC codes have a good hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ATC\n",
    "\"\"\"\n",
    "\n",
    "def build_atc_tree(unique_codes: list[str]) -> Tuple[list[str], Vocab]:\n",
    "    paths = []\n",
    "    atc_vocab = Vocab()\n",
    "    atc_leaf_vocab = Vocab()\n",
    "\n",
    "    root_node = \"atc_root\"\n",
    "    for code in unique_codes:\n",
    "        atc_leaf_vocab.add_sentence([code])\n",
    "        path = [code] + [code[:i] for i in [4, 3, 1]] + [root_node]\n",
    "\n",
    "        atc_vocab.add_sentence(path)\n",
    "        paths.append(path)\n",
    "\n",
    "    return paths, atc_vocab, atc_leaf_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions to create globally available <i>\"Ontology\"</i> and vocab over Unique (ATC/ICD) codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_atc_ontology():\n",
    "    unique_atc_codes = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ATC_CSV))[\"ATC4\"]\n",
    "    return build_atc_tree(unique_atc_codes.values.tolist())\n",
    "\n",
    "def _get_icd_ontology():\n",
    "    unique_icd_codes = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, UNIQUE_ICD_CSV))[\"ICD9_CODE\"]\n",
    "    return build_icd9_tree(unique_icd_codes.values.tolist())\n",
    "\n",
    "# TODO: icd codes in my parse are 6984 but acc to paper it should have been 1997 ==> prof suggested that we should see stats esp based on freq\n",
    "# TODO: atc codes in my parse are 413 but acc to paper it should have been 323\n",
    "GLOBAL_ATC_GRAPH, GLOBAL_ATC_VOCAB, GLOBAL_ATC_LEAF_VOCAB = _get_atc_ontology()\n",
    "GLOBAL_ICD_GRAPH, GLOBAL_ICD_VOCAB, GLOBAL_ICD_LEAF_VOCAB = _get_icd_ontology()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add special token to vocab. We are adding in the end so that they don't interfere with masking and multi-hot to code-list conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_ATC_VOCAB.add_special_tokens()\n",
    "GLOBAL_ICD_VOCAB.add_special_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-3: Building Ontology Embedding over ICD and ATC (using Ontology from prev part and apply attention over it)</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ontology embedding as described in <a href=\"https://arxiv.org/pdf/1906.00346.pdf\">GBERT paper</a>, we would need several components:\n",
    "<ol>\n",
    "    <li>Utility to build stage 1 edges. That is edge from direct child to parent </li>\n",
    "    <li>Utility to build stage 2 edges. That is edge from ancestor of a leaf node to the lead node itself </li>\n",
    "    <li>Graph Neural Network which are attention based message passing graph convolution neural network(GAT) </li>\n",
    "</ol>\n",
    "\n",
    "For more details about stage based edges, one should refer the main GBERT paper. Moreover, details about GAT can be found here https://arxiv.org/abs/1710.10903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> <u>Step 1. and 2.</u></h5>\n",
    "Assuming you have knowledge of what are stage-1/2 edges, we would go about how to create them now.\n",
    "\n",
    "We are using pytorch geometric for GNN. They have <a href=\"https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.message_passing.MessagePassing\">Message Passing GNN</a> which takes in \"flow\" param. This param defines how message \"flows\" between the nodes. For flow=\"source_to_target\" message flow from neighbor(j^th node) to target(i^th node). \n",
    "\n",
    "Thus one has to be very careful about how \"edge_index\"(input to Pytorch Geometric based GNNs) is formed. First of all, pytorch-geometric based Modules require COO based edge-index input. [One can google about them, here is the summary along with other formats that might be used somewhere else: https://scipy-lectures.org/advanced/scipy_sparse/storage_schemes.html#summary]\n",
    "\n",
    "Now coming back to step/stage-1, we would create [[parent indices], [direct-child indices]], since we want i = parent and j = direct child in this stage. (i.e. child to parent message flow, given flow=\"source_to_target\")\n",
    "\n",
    "For step/stage-2 we would create [[leaf indices], [ancestor indices]], since we want i = leaf and j = ancestor in stage-2. (i.e. ancestor to leaf message flow, given flow=\"source_to_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_one_edges(paths: list[str], graph_voc: Vocab) -> list[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "        :param paths: different paths from leaf node to root node in an ontology(icd/atc)\n",
    "        :param graph_voc: Vocab object for that ontology\n",
    "        :return: edge_idx: in COO format (more about that in pytorch docs or may refer here https://scipy-lectures.org/advanced/scipy_sparse/coo_matrix.html)\n",
    "    \"\"\"\n",
    "    edge_idx = []\n",
    "    for path in paths:\n",
    "        path_in_idx_format = list(map(lambda x: graph_voc.word2idx[x], path))\n",
    "        for i in range(len(path_in_idx_format) - 1):\n",
    "            # only direct children edges\n",
    "            # edge : parent(i)->child(j) [assuming flow j->i]\n",
    "            parent = path_in_idx_format[i+1]\n",
    "            node = path_in_idx_format[i]\n",
    "            edge_idx.append((parent, node))\n",
    "\n",
    "    edge_idx = list(dict.fromkeys(edge_idx))\n",
    "    row = list(map(lambda x: x[0], edge_idx))\n",
    "    col = list(map(lambda x: x[1], edge_idx))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def build_stage_two_edges(paths: list[str], graph_voc: Vocab) -> list[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "        :param paths: different paths from leaf node to root node in an ontology(icd/atc)\n",
    "        :param graph_voc: Vocab object for that ontology\n",
    "        :return: edge_idx: in COO format (more about that in pytorch docs or may refer here https://scipy-lectures.org/advanced/scipy_sparse/coo_matrix.html)\n",
    "    \"\"\"\n",
    "    edge_idx = []\n",
    "    for path in paths:\n",
    "        path_in_idx_format = list(map(lambda x: graph_voc.word2idx[x], path))\n",
    "        # leaf node to ancestor edges \n",
    "        # edge: leaf-node(i)->ancestors(j) [assuming flow j->i]\n",
    "        leaf = path_in_idx_format[0]\n",
    "        for i in range(1, len(path_in_idx_format)):\n",
    "            ancestor = path_in_idx_format[i]\n",
    "            edge_idx.append((leaf, ancestor))\n",
    "\n",
    "    edge_idx = list(dict.fromkeys(edge_idx))\n",
    "    row = list(map(lambda x: x[0], edge_idx))\n",
    "    col = list(map(lambda x: x[1], edge_idx))\n",
    "    return [row, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> <u>Step 3.</u></h5>\n",
    "We would use Pytorch Geometric GAT Convolutional layer and then use that module for creating General Embedding module ==> Would be called <i>OntologyEmbedding</i> that can be used for ICD/ATC embeddings over a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining General Embedding module ==> <i>OntologyEmbedding</i>. This class, as mentioned earlier, can be used to represent ICD/ATC embeddings in later stages where we implement BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "        get_ontology: method that will give the ontology on which to build embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, ontology_paths: Tuple[list[list[str]]], vocab: Vocab, emb_size=GAT_CONV_OUT_CHANNEL):\n",
    "        super(OntologyEmbedding, self).__init__()\n",
    "\n",
    "        stage_one_edges = build_stage_one_edges(ontology_paths, vocab)\n",
    "        stage_two_edges = build_stage_two_edges(ontology_paths, vocab)\n",
    "\n",
    "        self.edges1 = torch.LongTensor(stage_one_edges)\n",
    "        # print(f\"[OntologyEmbedding:init] shape of edge_index: {self.edges1.shape}\")\n",
    "        self.edges2 = torch.LongTensor(stage_two_edges)\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.in_channels = GAT_CONV_IN_CHANNEL\n",
    "        self.heads = GAT_CONV_HEADS\n",
    "        assert self.in_channels == self.heads * emb_size\n",
    "        self.g = GATConv(in_channels=self.in_channels, out_channels=emb_size, heads=self.heads, \\\n",
    "            dropout=GAT_CONV_DROPOUT, negative_slope=GAT_CONV_NEGATIVE_SLOP)\n",
    "\n",
    "        num_nodes = len(vocab.word2idx)\n",
    "        # TODO: Need to check that does special token gradient is happening\n",
    "        self.initial_embedding = nn.Parameter(torch.empty((num_nodes, self.in_channels)))\n",
    "        glorot(self.initial_embedding)\n",
    "\n",
    "    def forward(self):\n",
    "        emb = self.initial_embedding\n",
    "        # print(f\"[OntologyEmbedding::forward] emb.shape:{emb.shape}\")\n",
    "        stage1 = self.g(emb, self.edges1)\n",
    "        # print(f\"[OntologyEmbedding::forward] stage1.shape:{stage1.shape}\")\n",
    "\n",
    "        # concatenation being done of heads that is why below should pass\n",
    "        assert stage1.shape[-1] == self.g.in_channels\n",
    "        \n",
    "        emb = self.g(stage1, self.edges2)\n",
    "        # print(f\"[OntologyEmbedding::forward] emb.shape:{emb.shape}\")\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-4: Defining GBERT Model</u>\n",
    "\n",
    "We would need data of medication(ATC) and diagnosis(ICD) from visits for training. So first step towards building GBERT would be to parse MIMIC-III data for single visit patient's medication and diagnosis codes that would be used in pre-training.\n",
    "\n",
    "Steps for creating single visit patient's pkl file:\n",
    "<ol>\n",
    "    <li>Load <a href=\"https://mimic.mit.edu/docs/iii/tables/diagnoses_icd/\">DIAGNOSIS_ICD.csv</a> along with <a href=\"https://mimic.mit.edu/docs/iii/tables/admissions/\">ADMISSIONS.csv</a>. For medications: Merge <a href=\"https://mimic.mit.edu/docs/iii/tables/prescriptions/\">PRESCRIPTIONS.csv</a> and mapping files i.e. NDC-RXCUI mapping and RXCUI-ATC</li>\n",
    "    <li>To get the single visit patients we should group by \"SUBJECT_ID\" and \"HADM_ID\" on ADMISSIONS.csv and then select \"SUBJECT_ID\" with only 1 \"HADM_ID\"</li>\n",
    "    <li>We should select these \"SUBJECT_ID\"[which we got in step-2 ] and icd codes[which we got in previous step-1], from DIAGNOSIS_ICD.csv </li>\n",
    "    <li>Similarly, We should select these \"SUBJECT_ID\"[which we got in step-2] and their Atc4 codes[which we got in step-1], from PRESCRIPTIONS.csv </li>\n",
    "    <li>Next, pickling after merging both Diagnosis and Medication Data frames </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_visit_pkl_file(outputPath: str) -> pd.DataFrame:\n",
    "    # step 1\n",
    "    visit_record_key = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "    diag_df = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, DIAGNOSIS_ICD))\n",
    "    print(\"[create_single_visit_pkl_file] Begining of Loading of PRESCRIPTIONS.csv\")\n",
    "    medications_df = merge_prescriptions_and_mapping_files()[visit_record_key + [\"ATC4\"]]\n",
    "    print(\"[create_single_visit_pkl_file] Loaded PRESCRIPTIONS.csv\")\n",
    "    admission_df = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, ADMISSIONS))[visit_record_key]\n",
    "\n",
    "    # step 2\n",
    "    sub_id_adm_count = admission_df.groupby(by=[\"SUBJECT_ID\"])[\"HADM_ID\"].apply(lambda _: len(set(_))).reset_index()\n",
    "    single_visit_sub_id = sub_id_adm_count[sub_id_adm_count[\"HADM_ID\"] == 1][\"SUBJECT_ID\"]\n",
    "    \n",
    "    # step 3\n",
    "    single_visit_diag = pd.merge(diag_df, single_visit_sub_id, on=[\"SUBJECT_ID\"]).dropna().drop_duplicates()\n",
    "    single_visit_diag = single_visit_diag[visit_record_key + [\"ICD9_CODE\"]].groupby(by=visit_record_key)[\"ICD9_CODE\"].apply(list).reset_index()    \n",
    "    # Columns of single_visit_diag would be [\"SUBJECT_ID\", \"HADM_ID\",list[\"ICD9_CODE\"]]\n",
    "\n",
    "    # step 4\n",
    "    single_visit_meds = pd.merge(medications_df, single_visit_sub_id, on=[\"SUBJECT_ID\"]).dropna().drop_duplicates()\n",
    "    single_visit_meds = single_visit_meds.groupby(by=visit_record_key)[\"ATC4\"].apply(list).reset_index() \n",
    "    # row of single_visit_meds would look like: [\"SUBJECT_ID\", \"HADM_ID\", list[\"ATC4\"]]\n",
    "\n",
    "    # step 5\n",
    "    final_df = pd.merge(single_visit_diag, single_visit_meds, on=visit_record_key)\n",
    "    final_df.to_pickle(outputPath)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create pkl file for single visit <br>\n",
    "<span style=\"color: #055BA6\">create_single_visit_pkl_file(os.path.join(GLOBAL_DATA_PATH, SINGLE_VISIT_PKL))</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_layers, heads, batch_first):\n",
    "        super(BERT, self).__init__()\n",
    "        self.transformerEncoderLayer = nn.TransformerEncoderLayer(d_model, heads, d_ff, batch_first=batch_first)\n",
    "        self.transformerEncoder = nn.TransformerEncoder(self.transformerEncoderLayer, num_layers)\n",
    "\n",
    "    def forward(self, x, mask)->torch.Tensor:\n",
    "        # print(f\"[BERT: forward] x.shape:{x.shape}, mask.shape{mask.shape}\")\n",
    "        return self.transformerEncoder(x, src_key_padding_mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a basic BERT. We will define GBERT which uses BERT to get \"CLS\" encoding that represents the input DX/RX codes and will be later used to pre-train GBERT or predict RX in final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBERT(nn.Module):\n",
    "    def __init__(self, embedding_size=100, d_model=300, d_ff=300, num_layers=2, heads=4, batch_first=True):\n",
    "        super(GBERT, self).__init__()\n",
    "        self.ingest_embeddings = nn.Linear(embedding_size, d_model)\n",
    "        self.bert = BERT(d_model, d_ff, num_layers, heads, batch_first)\n",
    "        # This linear layer is to be applied on top of \"CLS\" of BERT output\n",
    "        self.cls = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, dx: torch.Tensor, dx_mask: torch.Tensor, rx: torch.Tensor, rx_mask: torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "            dx shape: [-1, max_seq_len, GAT_CONV_HEADS*GAT_CONV_OUT_CHANNEL]\n",
    "            dx_mask: [-1, max_seq_len]\n",
    "            rx shape: [-1, max_seq_len, GAT_CONV_HEADS*GAT_CONV_OUT_CHANNEL]\n",
    "            rx_mask: [-1, max_seq_len]\n",
    "        \"\"\"\n",
    "        dx = self.ingest_embeddings(dx) # simple since we don't have positional encodings\n",
    "        rx = self.ingest_embeddings(rx)\n",
    "        vd = self.bert(dx, dx_mask)\n",
    "        # print(f\"[GBERT::forward] vd[:, 0:1, :].shape: {vd[:, 0:1, :].shape}\")\n",
    "        vm = self.bert(rx, rx_mask)\n",
    "        # print(f\"[GBERT::forward] vm[:, 0:1, :].shape: {vm[:, 0:1, :].shape}\")\n",
    "\n",
    "        return self.cls(vd[:, 0:1, :].squeeze(dim=1)), self.cls(vm[:, 0:1, :].squeeze(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-5: Dataset and CollatedModel for Pre-training GBERT Models</u>\n",
    "\n",
    "We define here data loading step which include:\n",
    "<ol>\n",
    "    <li>Creating Padded list of ATC/ICD codes and mask tensor for padded codes. Will have different for ATC4 and ICD9_CODE</li>\n",
    "    <li> We also define multi-hot label that will be used as gold-standard or the target tensor</li>\n",
    "    <li>[Masking Strategy]: We will be masking with 15% probability - randomly </li>\n",
    "</ol>\n",
    "\n",
    "After that we will work on pre-training.\n",
    "\n",
    "<span style=\"color: #FF540D\">[Assuming] that paper means by single visit as Single Hospital admission. But in git implementation of G-BERT it seem to be first 24 hrs too. </span> Moreover Paper have max seq length of 62 but since We have whole admission(and not first 24 hrs) that may be the reason, so Subject_ID#96232 still have 100 ATC4 codes. Will need to revisit this since this will impact in defining max_seq_length of BERT too.\n",
    "<span style=\"color: #FF540D\">[Issue in author dataset] SUBJECT_ID 11, 86, 92 are not present in single visit. And no reasoning</span>\n",
    "For now We are sticking with 100 ==> 101 due to \"[CLS]\" for ATC4 and 40 for ICD9_CODE\n",
    "\n",
    "<span style=\"color: #FF540D\">[Issue in paper]: 15% of codes might not mean masking with probability 15%</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainingDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, atc_vocab: Vocab, atc_leaf_vocab: Vocab, icd_vocab: Vocab, icd_leaf_vocab: Vocab, max_atc_len: int, \\\n",
    "        max_icd_len: int):\n",
    "\n",
    "        super(PreTrainingDataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.maxAtcLen = max_atc_len\n",
    "        self.maxIcdLen = max_icd_len\n",
    "        self.atcVocab = atc_vocab\n",
    "        self.atcLeafVocab = atc_leaf_vocab\n",
    "        self.icdVocab = icd_vocab\n",
    "        self.icdLeafVocab = icd_leaf_vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def Build(self):\n",
    "        # Refer part-5 why max len -> 101, 40\n",
    "        max_atc_len =  PRETRAINING_MAX_ATC_LEN \n",
    "        max_icd_len =  PRETRAINING_MAX_ICD_LEN \n",
    "        single_visit: pd.DataFrame = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, SINGLE_VISIT_PKL))\n",
    "\n",
    "        # print(\"[PreTrainingDataset::Build] type \", first_visit_diag[\"ICD9_CODE\"].apply(len).max())\n",
    "        assert max_icd_len >= single_visit[\"ICD9_CODE\"].apply(len).max(), \"[PreTrainingDataset::Build] Max Seq length is less\"\n",
    "        assert max_atc_len >= single_visit[\"ATC4\"].apply(len).max(), \"[PreTrainingDataset::Build] Max Seq length is less\"\n",
    "        \n",
    "        return PreTrainingDataset(single_visit, GLOBAL_ATC_VOCAB, GLOBAL_ATC_LEAF_VOCAB, GLOBAL_ICD_VOCAB, GLOBAL_ICD_LEAF_VOCAB, max_atc_len, \\\n",
    "            max_icd_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        # self.df[\"SUBJECT_ID\"].size\n",
    "        # can see if the below works or not, if not then use above one\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "            can pad and covert to tensor later as well(collate_fn), if any issue here then can try that way\n",
    "        \"\"\"\n",
    "        atc:list[str] = self.df.iloc[index][\"ATC4\"]\n",
    "        icd:list[str] = self.df.iloc[index][\"ICD9_CODE\"]\n",
    "        \n",
    "        atc_word2idx_mask = torch.ones(self.maxAtcLen, dtype=torch.bool, device=DEVICE) # ones due to pytorch convention\n",
    "        atc_word2idx_list = torch.zeros(self.maxAtcLen, dtype=torch.long, device=DEVICE)\n",
    "        # CLS:\n",
    "        atc_word2idx_mask[0] = 0\n",
    "        atc_word2idx_list[0] = self.atcVocab.word2idx.get(\"[CLS]\")\n",
    "        atc_mlm_mask = torch.rand(len(atc)) < 0.15\n",
    "        for i, atc_code in enumerate(atc):\n",
    "            if atc_mlm_mask[i] == True:\n",
    "                idx = self.atcVocab.word2idx.get(\"[MASK]\") # need this idx to be over all graph nodes\n",
    "            else:\n",
    "                idx = self.atcVocab.word2idx.get(atc_code) # need this idx to be over all graph nodes\n",
    "            if idx == None:\n",
    "                print(\"PreTrainingDataset: Some unseen ATC code\", atc_code)\n",
    "            else:\n",
    "                atc_word2idx_mask[i+1] = 0\n",
    "                atc_word2idx_list[i+1] = idx\n",
    "\n",
    "\n",
    "        icd_word2idx_mask = torch.ones(self.maxIcdLen, dtype=torch.bool, device=DEVICE)\n",
    "        icd_word2idx_list = torch.zeros(self.maxIcdLen, dtype=torch.long, device=DEVICE)\n",
    "        # CLS:\n",
    "        icd_word2idx_mask[0] = 0\n",
    "        icd_word2idx_list[0] = self.icdVocab.word2idx.get(\"[CLS]\")\n",
    "        icd_mlm_mask = torch.rand(len(icd)) < 0.15\n",
    "        for i, icd_code in enumerate(icd):\n",
    "            if icd_mlm_mask[i] == True:\n",
    "                idx = self.icdVocab.word2idx.get(\"[MASK]\")\n",
    "            else:\n",
    "                idx = self.icdVocab.word2idx.get(icd_code)\n",
    "            icd_word2idx_mask[i+1] = 0\n",
    "            if idx == None:\n",
    "                # TODO: check if this is fine ==> one way can be to check if single visit pkl from author code has unk icds\n",
    "                icd_word2idx_list[i+1] = self.icdVocab.word2idx.get(\"[UNK]\")\n",
    "                # print(\"PreTrainingDataset: Some unseen icd code\", icd_code)\n",
    "                # print(\"[PreTrainingDataset]: Skipping this for now\")\n",
    "            else:\n",
    "                icd_word2idx_list[i+1] = idx\n",
    "    \n",
    "        return atc_word2idx_list, get_multi_hot(atc, self.atcLeafVocab), atc_word2idx_mask,\\\n",
    "            icd_word2idx_list, get_multi_hot(icd, self.icdLeafVocab), icd_word2idx_mask \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-training of GBERT is inspired from NLP based tasks such as NSP and MLM. However, to fit into medical domain, authors have changed them to Self-prediction and Double-prediction task(More details in paper). \n",
    "\n",
    "For above mentioned tasks we need 4 linear layers. \n",
    "<ul>\n",
    "    <li>Layer 1: converts Vd(output of BERT) to Diagnosis labels(i.e. self prediction)</li>\n",
    "    <li>Layer 2: converts Vd to Medication Labels(i.e. double prediction)</li>\n",
    "    <li>Layer 3: converts Vm(output of BERT) to Diagnosis labels(i.e. double prediction)</li>\n",
    "    <li>Layer 4: converts Vm to Medication Labels(i.e. self prediction)</li>\n",
    "</ul>\n",
    "We define a template for the above layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_task_layer_template = lambda d_model, pretraining_ff_model, vocab_size:\\\n",
    "    nn.Sequential(nn.Linear(d_model, pretraining_ff_model), nn.ReLU(), nn.Linear(pretraining_ff_model, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now integrate all the tools build before: GBERT, OntologyEmbedding and these 4 layers into one single model.\n",
    "Pre-training is done on this final model and the loss is added and propagated backward. \n",
    "\n",
    "Notice that data loading gives a list of codes and mask tensor. We obtain Ontology embedding[PART-3] inside the collated Model. And pass that embedding as input to BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollatedModelForPretraining(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CollatedModelForPretraining, self).__init__()\n",
    "        self.atc_ontology_embedding = OntologyEmbedding(GLOBAL_ATC_GRAPH, GLOBAL_ATC_VOCAB)\n",
    "        self.icd_ontology_embedding = OntologyEmbedding(GLOBAL_ICD_GRAPH, GLOBAL_ICD_VOCAB)\n",
    "\n",
    "        self.gbert = GBERT(embedding_size=GAT_CONV_HEADS*GAT_CONV_OUT_CHANNEL, d_model=BERT_IN_OUT, \\\n",
    "            d_ff=BERT_HIDDEN, num_layers=BERT_NUM_LAYERS, heads=BERT_NUM_HEAD_EACH_LAYER)\n",
    "\n",
    "        self.gbert_pretraining_prediction_layers = nn.ModuleList([\\\n",
    "            pretraining_task_layer_template(BERT_IN_OUT, PRETRAINING_FF_HIDDEN, len(GLOBAL_ICD_LEAF_VOCAB.idx2word)),\\\n",
    "            pretraining_task_layer_template(BERT_IN_OUT, PRETRAINING_FF_HIDDEN, len(GLOBAL_ATC_LEAF_VOCAB.idx2word)),\\\n",
    "            pretraining_task_layer_template(BERT_IN_OUT, PRETRAINING_FF_HIDDEN, len(GLOBAL_ICD_LEAF_VOCAB.idx2word)),\\\n",
    "            pretraining_task_layer_template(BERT_IN_OUT, PRETRAINING_FF_HIDDEN, len(GLOBAL_ATC_LEAF_VOCAB.idx2word))])\n",
    "    \n",
    "    def forward(self, atc: torch.Tensor, atc_mask: torch.Tensor, icd: torch.Tensor, icd_mask: torch.Tensor):\n",
    "        # self.atc_ontology_embedding(): [num_nodes in graph = vocab words(contain special tokens), out_channels = 100]\n",
    "        # atc list contains: idx and special token like \"[CLS]\" etc\n",
    "        atc_ontology_emb = self.atc_ontology_embedding()[atc]\n",
    "        icd_ontology_emb = self.icd_ontology_embedding()[icd]\n",
    "        vd, vm = self.gbert(atc_ontology_emb, atc_mask, icd_ontology_emb, icd_mask) # mask => seq_pad_mask\n",
    "\n",
    "        assert vd.shape[1] == BERT_IN_OUT and vm.shape[1] == BERT_IN_OUT, \"Bert should return only cls embedding\"\n",
    "\n",
    "        vd2dx, vd2rx, vm2dx, vm2rx = self.gbert_pretraining_prediction_layers[0](vd), self.gbert_pretraining_prediction_layers[1](vd),\\\n",
    "            self.gbert_pretraining_prediction_layers[2](vm), self.gbert_pretraining_prediction_layers[3](vm)\n",
    "        return torch.sigmoid(vd2dx), torch.sigmoid(vd2rx), torch.sigmoid(vm2dx), torch.sigmoid(vm2rx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-6: Pre-training Loop</u>\n",
    "\n",
    "Below will start pre-training of the GBERT model built and the OntologyEmbedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraining_loop(load_from_disk: bool = False):    \n",
    "    pretrain_data_loader = DataLoader(PreTrainingDataset.Build(), batch_size=PRETRAINING_BATCH_SIZE, shuffle=True)\n",
    "    pretraining_model = CollatedModelForPretraining().to(DEVICE)\n",
    "    if load_from_disk:\n",
    "        checkpoint = torch.load(os.path.join(GLOBAL_MODELS_PATH, PRETRAINING_MODEL))\n",
    "        pretraining_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    # print(\"[pretraining_loop]: Num of params: \", sum(p.numel() for p in pretraining_model.parameters()))\n",
    "    pretraining_optimizer = Adam(pretraining_model.parameters(), lr=0.001)\n",
    "    pretrain_criterion = nn.BCELoss()\n",
    "    pretraining_model.train(mode=True)\n",
    "    for epoch in range(PRETRAINING_EPOCH):\n",
    "        # print(f\"[pretraining_loop]: epoch: {epoch}\")\n",
    "        # print(\"===================================\")\n",
    "        pretraining_model.train()\n",
    "        jaccard = []\n",
    "        pr_auc = []\n",
    "        tqdm_data_loader = tqdm(pretrain_data_loader)\n",
    "        for data in tqdm_data_loader:\n",
    "            pretraining_optimizer.zero_grad()\n",
    "            atc_list, atc_labels, atc_mask, icd_list, icd_labels, icd_mask = data\n",
    "            vd2dx, vd2rx, vm2dx, vm2rx = pretraining_model(atc_list, atc_mask, icd_list, icd_mask)\n",
    "            # print(f\"vm2dx.shape: {vm2dx.shape}, icd_labels.shape: {icd_labels.shape}\")\n",
    "            loss = pretrain_criterion(vd2dx, icd_labels) + pretrain_criterion(vd2rx, atc_labels) + \\\n",
    "                pretrain_criterion(vm2dx, icd_labels) + pretrain_criterion(vm2rx, atc_labels)\n",
    "            loss.backward()\n",
    "            tqdm_data_loader.set_postfix({f\"[pretraining_loop] epoch:{epoch}:: loss\":str(loss.item())})\n",
    "            \n",
    "            y_pred = torch.zeros(vm2rx.shape).to(DEVICE)\n",
    "            y_pred[vm2rx > 0.5] = 1\n",
    "            jaccard_items, pr_auc_item = multi_label_metric(atc_labels, y_pred, vm2rx)[:2]\n",
    "            jaccard.append(jaccard_items)\n",
    "            pr_auc.append(pr_auc_item)\n",
    "            # print(\"[pretraining_loop]: j, pr_auc, f\", multi_label_metric(atc_labels, y_pred, vm2rx))\n",
    "            \n",
    "            pretraining_optimizer.step()\n",
    "\n",
    "        print(\"[pretraining_loop] Jaccard for this epoch:\", sum(jaccard)/len(jaccard))\n",
    "        print(\"[pretraining_loop] PR-AUC for this epoch:\", sum(pr_auc)/len(pr_auc))\n",
    "    pretraining_model.train(mode=False)\n",
    "    return pretraining_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to start pre-training <br>\n",
    "<span style=\"color: #055BA6\">pretrain_model = pretraining_loop()</span>\n",
    "\n",
    "Saving the pre-training model <br>\n",
    "<span style=\"color: #055BA6\">\n",
    "torch.save({ <br>\n",
    "    \"model_state_dict\": pretrain_model.state_dict() <br>\n",
    "    }, os.path.join(GLOBAL_MODELS_PATH, PRETRAINING_MODEL)) <br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-7: Preparing Dataset for Training</u>\n",
    "\n",
    "Since by now we are well versed with MIMIC-III, I would go directly to main points and point out that for multi-visit data pkl files we need these columns: \"SUBJECT_ID\", \"HADM_ID\", \"ICD9_CODE\", \"ATC4\". Multi-visit is actually dataset of patients with >= 1 visit and their ICD9 codes and ATC4 codes.\n",
    "\n",
    "But here \"ICD9_CODE\" and \"ATC4\" columns are actually list of codes. \n",
    "\n",
    "<ol>\n",
    "    <li>Read DIAGNOSIS_ICD.csv MIMIC-III file and process to get list format of ICD9_CODE </li>\n",
    "    <li>Read PRESCRIPTIONS.csv MIMIC-III file and process to get list format of ATC4 codes </li>\n",
    "    <li>Merge two above Data frames </li>\n",
    "    <li>Pickle merged Data frame</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_visit_pkl_files(outputPath: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # step 1\n",
    "    diag_df = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, DIAGNOSIS_ICD)).dropna().drop_duplicates()\n",
    "    diag_df = diag_df.groupby([\"SUBJECT_ID\", \"HADM_ID\"])[\"ICD9_CODE\"].apply(list).reset_index()\n",
    "    \n",
    "    # step 2\n",
    "    medications = merge_prescriptions_and_mapping_files()[[\"SUBJECT_ID\", \"HADM_ID\", \"ATC4\"]].dropna().drop_duplicates()\n",
    "    print(\"[create_multi_visit_pkl_files] Got medication DF\")\n",
    "    medications = medications.groupby([\"SUBJECT_ID\", \"HADM_ID\"])[\"ATC4\"].apply(list).reset_index()\n",
    "\n",
    "    # step 3\n",
    "    merged_entity = pd.merge(medications, diag_df, on=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    print(\"[create_multi_visit_pkl_files] Merging of Medications and Diagnosis code done\")\n",
    "\n",
    "    # step 4\n",
    "    merged_entity.to_pickle(outputPath)\n",
    "    return merged_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to pkl multi-visit data <br>\n",
    "<span style=\"color: #055BA6\">create_multi_visit_pkl_files(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_PKL))</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since we need all previous visits per visit, for training input, we will create another file which we call MULTI_VISIT_TEMPORAL_PKL. This file's each row contains \"SUBJECT_ID\", \"HADM_ID\", list of list of ICD codes, list of list of ATC codes.\n",
    "\n",
    "The list of list column is basically list of ICDs/ATC4 which appear before and during that visit. Note patients with >= 2 hospital admission are considered.\n",
    "\n",
    "steps for creating this file would be:\n",
    "<ol>\n",
    "    <li>Load MULTI_VISIT_PKL, ADMISSIONS.csv and SINGLE_VISIT_PKL file for processing. Remove single visit patient from ADMISSIONS data frame</li>\n",
    "    <li>Find out from ADMISSIONS.csv HADM_ID's which are for given SUBJECT_ID and are temporally before the given HADM_ID </li>\n",
    "    <li>Now merge MULTI_VISIT_PKL with the Data frame obtained above. This would give us \"All the HADM_IDs in temporal order with ATC4 and ICD9_CODE of temporally < current HADM_ID\". We call the new Data frame obtained as \"multi_visit_temporal\"</li>\n",
    "    <li>For simplification, we then disintegrate \"multi_visit_temporal\" DF obtained from prev step. This helps in applying list method(this can be improved). Disintegrated DFs are for ICD9_CODE and ATC4 codes. We merge these two DFs to get \"multi_visit_temporal\"(order not reqd between ICD9_CODE and ATC4)</li>\n",
    "    <li>After above processing we have codes for visits < HADM_ID of each row. So we will again join with MULTI_VISIT_PKL to get the latest one codes available too and insert at 0th place for the list of ICD9_CODE obtained so far. </li>\n",
    "    <li>check and report error if any length 1 array in ICD9_CODE or ATC4 code present</li>\n",
    "    <li>Now we need to add another factor which we will call Loss factor. This factor is \"1/(T-1)\" present in equation (10) of the G-BERT paper. This would scale the training loss properly.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_visit_temporal_pkl_files(outputPath: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # step 1\n",
    "    visit_record_key = [\"SUBJECT_ID\", \"HADM_ID\", \"ICD9_CODE\", \"ATC4\"]\n",
    "    multi_visit = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_PKL))[visit_record_key[1:]]\n",
    "    admission_df = pd.read_csv(os.path.join(GLOBAL_DATA_PATH, ADMISSIONS))[visit_record_key[:2] + [\"ADMITTIME\"]]\n",
    "    single_visit_sub_id = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, SINGLE_VISIT_PKL))[\"SUBJECT_ID\"]\n",
    "    admission_df = admission_df.loc[~admission_df[\"SUBJECT_ID\"].isin(single_visit_sub_id)]\n",
    "    \n",
    "    # Step 2\n",
    "    self_joined = pd.merge(admission_df, admission_df, on=[\"SUBJECT_ID\"])\n",
    "    self_joined_admission_df = self_joined.loc[self_joined[\"ADMITTIME_x\"] > self_joined[\"ADMITTIME_y\"]]\n",
    "    # HADM_ID_y/HADM_ID -- represents temporally < current_HADM_ID\n",
    "    self_joined_admission_df = self_joined_admission_df.rename(columns={\"HADM_ID_y\":\"HADM_ID\", \"HADM_ID_x\":\"current_HADM_ID\"})\n",
    "    # Columns: SUBJECT_ID, current_HADM_ID, ADMITTIME_x, HADM_ID, ADMITTIME_y\n",
    "\n",
    "    # Step 3\n",
    "    multi_visit_temporal = pd.merge(self_joined_admission_df, multi_visit, on=[\"HADM_ID\"])\n",
    "    multi_visit_temporal = multi_visit_temporal.drop(columns=[\"HADM_ID\"])\n",
    "    multi_visit_temporal = multi_visit_temporal.rename(columns={\"current_HADM_ID\": \"HADM_ID\"})\n",
    "\n",
    "    # Step 4\n",
    "    temporal_icd = multi_visit_temporal[visit_record_key[:3]].groupby(by=[\"SUBJECT_ID\", \"HADM_ID\"])[\"ICD9_CODE\"].apply(list).reset_index()\n",
    "    temporal_atc = multi_visit_temporal[visit_record_key[:2] + [\"ATC4\"]].groupby(by=[\"SUBJECT_ID\", \"HADM_ID\"])[\"ATC4\"].apply(list).reset_index()\n",
    "    multi_visit_temporal = pd.merge(temporal_icd, temporal_atc, on=[\"SUBJECT_ID\", \"HADM_ID\"])   \n",
    "    \n",
    "    # Step 5\n",
    "    multi_visit_temporal = pd.merge(multi_visit_temporal, multi_visit, on=[\"HADM_ID\"], suffixes=(\"\", \"_curr\"))\n",
    "    multi_visit_temporal.apply(lambda row: row[\"ICD9_CODE\"].insert(0, row[\"ICD9_CODE_curr\"]), axis=1)\n",
    "    multi_visit_temporal.apply(lambda row: row[\"ATC4\"].insert(0, row[\"ATC4_curr\"]), axis=1)\n",
    "\n",
    "    # Step 6\n",
    "    multi_visit_temporal = multi_visit_temporal.drop(columns=[\"ICD9_CODE_curr\"])\n",
    "    multi_visit_temporal = multi_visit_temporal.drop(columns=[\"ATC4_curr\"])\n",
    "    def row_checker(row):\n",
    "        assert len(row[\"ICD9_CODE\"]) > 1 and len(row[\"ATC4\"]) > 1, \"[create_multi_visit_temporal_pkl_files] Some issue in data\"\n",
    "    multi_visit_temporal.apply(row_checker, axis=1)\n",
    "\n",
    "    # Step 7\n",
    "    T_minus_one = multi_visit_temporal[\"SUBJECT_ID\"]\n",
    "    T_minus_one = T_minus_one.groupby(T_minus_one).count().to_frame(name=\"T_1\").reset_index()\n",
    "    multi_visit_temporal = pd.merge(multi_visit_temporal, T_minus_one, on=[\"SUBJECT_ID\"])\n",
    "\n",
    "    multi_visit_temporal.to_pickle(outputPath)\n",
    "\n",
    "    return multi_visit_temporal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to pkl multi-visit data <br>\n",
    "<span style=\"color: #055BA6\">create_multi_visit_temporal_pkl_files(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_TEMPORAL_PKL))</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-8: Dataset and CollatedModel for Fine-tuning (training)</u>\n",
    "To get final training/validation and testing procedures we need to proceed in step wise fashion.\n",
    "\n",
    "#### <u>Step 1:</u>\n",
    "Now similar to previous steps of pre-training, here we would define a training data-set class. \n",
    "\n",
    "Steps to create training data-set:\n",
    "<ol>\n",
    "    <li>Use Dataset class(from pytorch) with Build method specifying MULTI_VISIT_TEMPORAL_PKL</li>\n",
    "    <li>for __getitem__ we would return list of list of padded sequence for both ICD9_CODE and ATC4 code</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDatasetWithoutMasking(Dataset):\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, atc_vocab: Vocab, atc_leaf_vocab: Vocab, icd_vocab: Vocab, icd_leaf_vocab: Vocab, max_atc_len: int, \\\n",
    "        max_icd_len: int):\n",
    "        super(TrainingDatasetWithoutMasking, self).__init__()\n",
    "        self.df = df\n",
    "        self.maxAtcLen = max_atc_len\n",
    "        self.maxIcdLen = max_icd_len\n",
    "        self.atcVocab = atc_vocab\n",
    "        self.atcLeafVocab = atc_leaf_vocab\n",
    "        self.icdVocab = icd_vocab\n",
    "        self.icdLeafVocab = icd_leaf_vocab\n",
    "    \n",
    "    @classmethod\n",
    "    def Build(self):\n",
    "        # testing max len -> 92 since ATC4 max for 100 was in actually single-visit only\n",
    "        max_atc_len = TRAINING_MAX_ATC_LEN # can also set ATC code max \n",
    "        max_icd_len = TRAINING_MAX_ICD_LEN # can also set ATC code max \n",
    "        multi_visit_temporal_df: pd.DataFrame = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_TEMPORAL_PKL))\n",
    "\n",
    "        # since now the __len__ contains indexing based on SUBJECT_ID\n",
    "        indexed = multi_visit_temporal_df[\"SUBJECT_ID\"]\n",
    "        indexed = indexed.groupby(indexed).count().to_frame(\"TRASH_COLUMN\").reset_index().drop(columns=[\"TRASH_COLUMN\"]).reset_index() # index and SUBJECT_ID\n",
    "        multi_visit_temporal_df = pd.merge(multi_visit_temporal_df, indexed, on=[\"SUBJECT_ID\"])\n",
    "\n",
    "        # print(\"[TrainingDatasetWithoutMasking::Build] type \", multi_visit_temporal_df[\"ICD9_CODE\"]\\\n",
    "        #     .apply(lambda item: max([len(ls) for ls in item])).max())\n",
    "        getMaxLengthOfListInList = lambda item: max([len(ls) for ls in item])\n",
    "        assert max_icd_len >= multi_visit_temporal_df[\"ICD9_CODE\"].apply(getMaxLengthOfListInList).max() and \\\n",
    "            max_atc_len >= multi_visit_temporal_df[\"ATC4\"].apply(getMaxLengthOfListInList).max(),\\\n",
    "                \"[TrainingDatasetWithoutMasking::Build] Max Seq length is less\"\n",
    "        \n",
    "        return TrainingDatasetWithoutMasking(multi_visit_temporal_df, GLOBAL_ATC_VOCAB, GLOBAL_ATC_LEAF_VOCAB, GLOBAL_ICD_VOCAB, \\\n",
    "            GLOBAL_ICD_LEAF_VOCAB, max_atc_len, max_icd_len)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # self.df.shape[0]\n",
    "        # can see if the below works or not, if not then use above one\n",
    "        return self.df[\"index\"].max() + 1 # need to change the logic of build for this\n",
    "    \n",
    "    def _process_(self, atc:list[list[str]], icd:list[list[str]]):\n",
    "        assert len(atc) == len(icd), \"Should have equal number of temporal visit\"\n",
    "        \n",
    "        atc_word2idx_mask = torch.ones((len(atc), self.maxAtcLen), dtype=torch.bool, device=DEVICE) # ones due to pytorch convention\n",
    "        atc_word2idx_list = torch.zeros((len(atc), self.maxAtcLen), dtype=torch.long, device=DEVICE)\n",
    "        # CLS:\n",
    "        atc_word2idx_mask[:, 0] = 0\n",
    "        atc_word2idx_list[:, 0] = self.atcVocab.word2idx.get(\"[CLS]\")\n",
    "        for i, atc_codes in enumerate(atc):\n",
    "            for j, atc_code in enumerate(atc_codes):\n",
    "                idx = self.atcVocab.word2idx.get(atc_code) # need this idx to be over all graph nodes\n",
    "                if idx == None:\n",
    "                    print(\"PreTrainingDataset: Some unseen ATC code\", atc_code)\n",
    "                else:\n",
    "                    atc_word2idx_mask[i, j+1] = 0\n",
    "                    atc_word2idx_list[i, j+1] = idx\n",
    "\n",
    "        icd_word2idx_mask = torch.ones((len(icd), self.maxIcdLen), dtype=torch.bool, device=DEVICE)\n",
    "        icd_word2idx_list = torch.zeros((len(icd), self.maxIcdLen), dtype=torch.long, device=DEVICE)\n",
    "        # CLS:\n",
    "        icd_word2idx_mask[:, 0] = 0\n",
    "        icd_word2idx_list[:, 0] = self.icdVocab.word2idx.get(\"[CLS]\")\n",
    "        for i, icd_codes in enumerate(icd):\n",
    "            for j, icd_code in enumerate(icd_codes):\n",
    "                idx = self.icdVocab.word2idx.get(icd_code)\n",
    "                icd_word2idx_mask[i, j+1] = 0\n",
    "                if idx == None:\n",
    "                    # print(\"PreTrainingDataset: Some unseen icd code\", icd_code)\n",
    "                    # We are going with [UNK] token since we need top 2k diagnosis only\n",
    "                    icd_word2idx_list[i, j+1] = self.icdVocab.word2idx.get(\"[UNK]\")\n",
    "                else:\n",
    "                    icd_word2idx_list[i, j+1] = idx\n",
    "    \n",
    "\n",
    "        return atc_word2idx_list, get_multi_hot(atc, self.atcLeafVocab), atc_word2idx_mask,\\\n",
    "            icd_word2idx_list, get_multi_hot(icd, self.icdLeafVocab), icd_word2idx_mask \n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "            index: represents a patient\n",
    "            0th index icd/atc always represent the most recent diagnosis/medication\n",
    "            can pad and covert to tensor later as well(collate_fn), if any issue here then can try that way\n",
    "        \"\"\"\n",
    "        patient_data: pd.DataFrame = self.df.loc[self.df[\"index\"] == index]\n",
    "        T_1 = patient_data[\"T_1\"].iloc[0]\n",
    "        atc_padded_seq = []\n",
    "        atc_seq_mask = []\n",
    "        atc_multi_hot = []\n",
    "        icd_padded_seq = []\n",
    "        icd_seq_mask = []\n",
    "        for _, row in patient_data.iterrows():\n",
    "            atc_seq, atc_mh, atc_mask, icd_seq, _, icd_mask = self._process_(row[\"ATC4\"], row[\"ICD9_CODE\"])\n",
    "            atc_padded_seq.append(atc_seq)\n",
    "            atc_multi_hot.append(atc_mh)\n",
    "            atc_seq_mask.append(atc_mask)\n",
    "\n",
    "            icd_padded_seq.append(icd_seq)\n",
    "            icd_seq_mask.append(icd_mask)\n",
    "\n",
    "        return atc_padded_seq, atc_multi_hot, atc_seq_mask, icd_padded_seq, icd_seq_mask, T_1\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Step 2:</u>\n",
    "In this step we define the transformation function as defined by equation 9 of paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = lambda d_model, vocab_size: nn.Sequential(nn.Linear(3*d_model, vocab_size), nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Step 3:</u>\n",
    "In this step we define a Collated Model just like we did in pre-training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollatedModelForTraining(nn.Module):\n",
    "    def __init__(self, load_prediction_layer: bool = False):\n",
    "        super(CollatedModelForTraining, self).__init__()\n",
    "        pretraining_model = CollatedModelForPretraining()\n",
    "        checkpoint = torch.load(os.path.join(GLOBAL_MODELS_PATH, PRETRAINING_MODEL))\n",
    "        pretraining_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.atc_ontology_embedding = pretraining_model.atc_ontology_embedding\n",
    "        self.icd_ontology_embedding = pretraining_model.icd_ontology_embedding\n",
    "        self.gbert = pretraining_model.gbert\n",
    "\n",
    "        if load_prediction_layer:\n",
    "            training_checkpoint = torch.load(os.path.join(GLOBAL_MODELS_PATH, TRAINING_MODEL))\n",
    "            # It would be recursive so never send True from here\n",
    "            dummy_train_model = CollatedModelForTraining(load_prediction_layer=False)\n",
    "            dummy_train_model.load_state_dict(training_checkpoint[\"model_state_dict\"])\n",
    "            self.prediction_layer = dummy_train_model.prediction_layer\n",
    "        else:\n",
    "            self.prediction_layer = prediction_layer(BERT_IN_OUT, len(GLOBAL_ATC_LEAF_VOCAB.idx2word))\n",
    "    \n",
    "    def forward(self, atc: torch.Tensor, atc_mask: torch.Tensor, icd: torch.Tensor, icd_mask: torch.Tensor):\n",
    "        # self.atc_ontology_embedding(): [num_nodes in graph = vocab words(contain special tokens), out_channels = 100]\n",
    "        # atc/icd list of list where each innermost list contains: idx and special token idx like \"[CLS]\" etc\n",
    "        assert len(icd) == len(atc), \"==> Input incorrect\"\n",
    "        icd_ontology_embeddings = self.icd_ontology_embedding()[icd]\n",
    "        atc_ontology_embeddings = self.atc_ontology_embedding()[atc]\n",
    "        assert len(icd) == len(icd_ontology_embeddings), \"==> Issue in embeddings\"\n",
    "        assert len(atc) == len(atc_ontology_embeddings), \"==> Issue in embeddings\"\n",
    "\n",
    "        vd, vm = self.gbert(atc_ontology_embeddings, atc_mask, icd_ontology_embeddings, icd_mask) # mask => seq_pad_mask\n",
    "        # 0th index represents the most latest, rest are random ordered\n",
    "        current_visit_vd = vd[0]\n",
    "\n",
    "        historical_visit_vd = torch.zeros(vd[0].shape)\n",
    "        historical_visit_vm = torch.zeros(vm[0].shape)\n",
    "        cnt = 0\n",
    "        for i in range(1, len(icd)):\n",
    "            cnt += 1\n",
    "            historical_visit_vd += vd[i]\n",
    "            historical_visit_vm += vm[i]\n",
    "\n",
    "        assert historical_visit_vd.shape[0] == BERT_IN_OUT and historical_visit_vm.shape[0] == BERT_IN_OUT, \"Bert should return only cls embedding\"\n",
    "        assert cnt > 0, \"Some issue in input\"\n",
    "        historical_visit_vd /= cnt\n",
    "        historical_visit_vm /= cnt\n",
    "\n",
    "        concat_visit = torch.cat((current_visit_vd, historical_visit_vd, historical_visit_vm), dim=0)\n",
    "\n",
    "        return self.prediction_layer(concat_visit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-9: Training Loop</u>\n",
    "\n",
    "Below will start \"training\" of the GBERT model built and the OntologyEmbedding model. In this step we would define training-validation-testing ratio . Acc to paper we have 0.6:0.2:0.2 ratio. So we will follow that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_dataloader()-> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    orig_data_set = TrainingDatasetWithoutMasking.Build()\n",
    "    split_for_train = int(0.6*len(orig_data_set))\n",
    "    split_for_validation = int(0.2*len(orig_data_set))\n",
    "\n",
    "    train_data_set, val_data_set, test_data_set = random_split(orig_data_set, [split_for_train, \\\n",
    "        split_for_validation, len(orig_data_set) - split_for_train - split_for_validation])\n",
    "\n",
    "    train_data_loader = DataLoader(train_data_set, batch_size=1, shuffle=True) # batch size is 1 since each \n",
    "    # row has list of list. If it is fast then will increase batch size ==> will need code changes\n",
    "    # will do it non conventionally ==> will loop batch times and loss.backward on 10 loops manually\n",
    "    # this will require minimum code changes. Otherwise I will have to work out matrices a lot\n",
    "    val_data_loader = DataLoader(val_data_set, batch_size=1) \n",
    "    test_data_loader = DataLoader(test_data_set, batch_size=1) \n",
    "\n",
    "    return train_data_loader, val_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a training loop like we did in pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingAndValidation_loop(data_loader: torch.utils.data.DataLoader, eval_mode: bool = False, \\\n",
    "    load_fine_tuning:bool = False):    \n",
    "\n",
    "    model = CollatedModelForTraining(load_fine_tuning).to(DEVICE)\n",
    "    if eval_mode:\n",
    "        checkpoint = torch.load(os.path.join(GLOBAL_MODELS_PATH, TRAINING_MODEL))\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    # print(\"[trainingAndValidation_loop]: model params\", sum(p.numel() for p in model.parameters()))\n",
    "    if eval_mode:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train(mode=True)\n",
    "        training_optimizer = Adam(model.parameters(), lr=0.001)\n",
    "        train_criterion = nn.BCELoss()\n",
    "    epoch = TRAINING_EPOCH if not eval_mode else 1\n",
    "    for epoch in range(epoch):\n",
    "        # print(f\"[trainingAndValidation_loop]: epoch: {epoch}\")\n",
    "        # print(\"===================================\")\n",
    "        epoch_jaccard_list = []\n",
    "        epoch_pr_auc_list = []\n",
    "        if not eval_mode:\n",
    "            epoch_loss = []\n",
    "        tqdm_iterator = tqdm(data_loader)\n",
    "        for patient_data in tqdm_iterator:\n",
    "            if not eval_mode:\n",
    "                training_optimizer.zero_grad()\n",
    "                patient_loss: torch.tensor = None\n",
    "            jaccard_list = []\n",
    "            pr_auc_list = []\n",
    "            atc_padded_list, atc_multi_hot_labels, atc_seq_mask, icd_padded_list, icd_seq_mask, t_1 = patient_data\n",
    "\n",
    "            for data_count in range(t_1):\n",
    "                atc_list = atc_padded_list[data_count].squeeze(dim=0)\n",
    "                atc_labels = atc_multi_hot_labels[data_count].squeeze(dim=0)\n",
    "                atc_mask = atc_seq_mask[data_count].squeeze(dim=0)\n",
    "\n",
    "                icd_list = icd_padded_list[data_count].squeeze(dim=0)\n",
    "                icd_mask = icd_seq_mask[data_count].squeeze(dim=0)\n",
    "\n",
    "                # print(f\"[trainingAndValidation_loop]: atc_list.shape: {atc_list.shape}, icd_list.shape: {icd_list.shape}\")\n",
    "                # print(f\"[trainingAndValidation_loop]: atc_labels.shape: {atc_labels.shape}\")\n",
    "                # print(f\"[trainingAndValidation_loop]: atc_mask.shape: {atc_mask.shape}, icd_mask.shape: {icd_mask.shape}\")\n",
    "\n",
    "                predicted_rx = model(atc_list, atc_mask, icd_list, icd_mask)\n",
    "\n",
    "                # print(f\"vm2dx.shape: {vm2dx.shape}, icd_labels.shape: {icd_labels.shape}\")\n",
    "                if not eval_mode:\n",
    "                    if patient_loss == None:\n",
    "                        patient_loss = train_criterion(predicted_rx, atc_labels[0])\n",
    "                    else: \n",
    "                        patient_loss += train_criterion(predicted_rx, atc_labels[0])\n",
    "                \n",
    "                y_pred = torch.zeros(predicted_rx.shape).to(DEVICE)\n",
    "                y_pred[predicted_rx > 0.5] = 1\n",
    "                # print(\"[trainingAndValidation_loop]: j, r, f, p, p_1, p_2, p_3, roc_auc \",\\\n",
    "                    #  multi_label_metric(atc_labels[0].unsqueeze(dim=0), y_pred.unsqueeze(dim=0), predicted_rx.unsqueeze(dim=0)))\n",
    "                jaccard_item, pr_auc_item = multi_label_metric(atc_labels[0].unsqueeze(dim=0), y_pred.unsqueeze(dim=0), predicted_rx.unsqueeze(dim=0))[:2]\n",
    "                jaccard_list.append(jaccard_item)\n",
    "                pr_auc_list.append(pr_auc_item)\n",
    "\n",
    "            if not eval_mode:           \n",
    "                patient_loss = patient_loss / t_1\n",
    "                patient_loss.backward()\n",
    "                training_optimizer.step()\n",
    "                epoch_loss.append(patient_loss.item())\n",
    "                tqdm_iterator.set_postfix({f\"[trainingAndValidation_loop] epoch:{epoch}:: patient loss\":str(patient_loss.item())})\n",
    "\n",
    "            patient_avg_jacc = sum(jaccard_list)/len(jaccard_list)\n",
    "            patient_avg_pr_auc = sum(pr_auc_list)/len(pr_auc_list)\n",
    "            epoch_jaccard_list.append(patient_avg_jacc)\n",
    "            epoch_pr_auc_list.append(patient_avg_pr_auc)\n",
    "        if not eval_mode:\n",
    "            print(\"[trainingAndValidation_loop]: Average loss for this epoch\", sum(epoch_loss)/len(epoch_loss))\n",
    "        print(\"[trainingAndValidation_loop]: Average jaccard for this epoch\", sum(epoch_jaccard_list)/len(epoch_jaccard_list))\n",
    "        print(\"[trainingAndValidation_loop]: Average PR-AUC for this epoch\", sum(epoch_pr_auc_list)/len(epoch_pr_auc_list))\n",
    "    if not eval_mode:\n",
    "        model.train(mode=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to start training <br>\n",
    "<span style=\"color: #055BA6\">\n",
    "train_loader, val_loader, test_loader = get_train_val_dataloader() <br>\n",
    "model = trainingAndValidation_loop(train_loader)\n",
    "</span>\n",
    "\n",
    "Saving the training model <br>\n",
    "<span style=\"color: #055BA6\">\n",
    "torch.save({ <br>\n",
    "    \"model_state_dict\": model.state_dict() <br>\n",
    "    }, os.path.join(GLOBAL_MODELS_PATH,TRAINING_MODEL)) <br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-10: Testing (and validation) Loop</u>\n",
    "\n",
    "Almost all the logic remains same in training_loop, main thing that changes is : rather than creating a new model we load it from the disk. For this we would simply pass a flag to <u><i>training_loop</i></u> and that flag would be <u><i>eval_mode: bool</i></u>\n",
    "\n",
    "Validation code --> <u><i>eval_mode: bool</i></u> is True here also. Just send different data_loader.\n",
    "\n",
    "Code to start evaluation <br>\n",
    "<span style=\"color: #055BA6\">\n",
    "train_loader, val_loader, test_loader = get_train_val_dataloader() <br>\n",
    "model = trainingAndValidation_loop(test_loader, eval_mode = True)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-11: Pre-Training, Training and Validation as per Original Paper setup </u>\n",
    "Author have mentioned to alternate 5 epochs between pre-training and fine-tuning(training) for 15 times to stabilize the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    def pre_train(load_from_disk:bool = False, iteration:int = 0):\n",
    "        print(f\"=================Iteration: {iteration}==============================\")\n",
    "        tracemalloc.start()\n",
    "        pretrain_model = pretraining_loop(load_from_disk)\n",
    "        _, peek = tracemalloc.get_traced_memory()\n",
    "        print(\"Peek memory during pre-training\", peek)\n",
    "        tracemalloc.stop()\n",
    "        torch.save({ \n",
    "            \"model_state_dict\": pretrain_model.state_dict() \n",
    "            }, os.path.join(GLOBAL_MODELS_PATH, PRETRAINING_MODEL)) \n",
    "\n",
    "    train_loader, _, test_loader = get_train_val_dataloader()\n",
    "    def train(load_fine_tuning:bool = False):\n",
    "        tracemalloc.start()\n",
    "        train_model = trainingAndValidation_loop(train_loader, load_fine_tuning=load_fine_tuning)\n",
    "        _, peek = tracemalloc.get_traced_memory()\n",
    "        print(\"Peek memory during training\", peek)\n",
    "        tracemalloc.stop()\n",
    "        torch.save({\n",
    "            \"model_state_dict\": train_model.state_dict()\n",
    "            }, os.path.join(GLOBAL_MODELS_PATH,TRAINING_MODEL))\n",
    "\n",
    "    # pre_train()\n",
    "    # train()\n",
    "    # for iteration in range(8, TRINING_ITERATION):\n",
    "        # pre_train(True, iteration)\n",
    "        # train(True)\n",
    "    train(True)\n",
    "\n",
    "    print(f\"=================Evaluation==============================\")\n",
    "    trainingAndValidation_loop(test_loader, load_fine_tuning=True, eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Part-xx: Baselines</u>\n",
    "\n",
    "#### <u>[I] Logistic Regression:</u>\n",
    "Input to logistic regression according to GBERT paper is multi-hot vector of each visit. Assuming that GBERT paper do not sum up previous visit multi-hot vectors and predicts only based on the current visit multi-hot vector.\n",
    "\n",
    "##### <u>Step 1: Data input:</u>\n",
    "<ol>\n",
    "    <li>We will use MULTI_VISIT_PKL and get ICD9_CODE and ATC4 code for each visit</li>\n",
    "    <li>We will convert visit to multi-hot vector and finally store them in numpy matrix</li>\n",
    "    <li>Ratio of breakdown is assumed to be 0.8 for training and 0.2 for test</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_for_logistic_regression()-> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_pickle(os.path.join(GLOBAL_DATA_PATH, MULTI_VISIT_PKL))\n",
    "    df[\"ICD9_CODE\"] = df[\"ICD9_CODE\"].apply(lambda ls: get_multi_hot(ls, GLOBAL_ICD_LEAF_VOCAB).numpy())\n",
    "    df[\"ATC4\"] = df[\"ATC4\"].apply(lambda ls: get_multi_hot(ls, GLOBAL_ATC_LEAF_VOCAB).numpy())\n",
    "    X = np.array(df[\"ICD9_CODE\"].values.tolist(), dtype=np.uint8)\n",
    "    Y = np.array(df[\"ATC4\"].values.tolist(), dtype=np.uint8)\n",
    "    index = int(0.8 * X.shape[0])\n",
    "    return X[:index],Y[:index], X[index:], Y[index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to load logistic regression data<br>\n",
    "<span style=\"color: #055BA6\">X_train, Y_train, X_test, Y_test = data_loader_for_logistic_regression()</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>Step 2: Logistic Regression:</u>\n",
    "<ol>\n",
    "    <li>We are using sklearn package to get one-vs-rest Binary relevance based Logistic regression with L2 regularization</li>\n",
    "    <li>TODO: Need to add grid search for optimal Logistic regression</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression():\n",
    "    X_train, Y_train, X_test, Y_test = data_loader_for_logistic_regression()\n",
    "    params = {\n",
    "        \"estimator__penalty\": [\"l2\"],\n",
    "        \"estimator__C\": np.linspace(0.00002, 1, 100)\n",
    "    }\n",
    "    clf = OneVsRestClassifier(LogisticRegression())\n",
    "    lr_gs = GridSearchCV(clf, params, verbose=1).fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = lr_gs.predict(X_test)\n",
    "    Y_pred_prob = lr_gs.predict_proba(X_test)\n",
    "    print(multi_label_metric(torch.from_numpy(Y_test), torch.from_numpy(Y_pred), torch.from_numpy(Y_pred_prob)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to load logistic regression data<br>\n",
    "<span style=\"color: #055BA6\">run_logistic_regression()</span>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6b480f4ea18601e70160c49282c8fadf5e45a8ee1f1ea69d57733a45cc61c3d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
